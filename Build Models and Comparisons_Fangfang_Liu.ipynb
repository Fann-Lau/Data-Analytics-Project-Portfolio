{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Build Models and Comparisons** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to study the relationship between economic characteristics and telephone fraud strategies, the appropriate independent variables should reflect various aspects of economic conditions.\n",
    "\n",
    "I choose the following aspects of economic data as independent variables:\n",
    "\n",
    "1. **GDP Per Capita**: measures the overall level of economic activity and wealth in a state or region. More economically developed areas may have more fraudulent activity because fraudsters may believe that residents of these areas have more money.\n",
    "\n",
    "2. **Personal Income**: includes total personal income and per capita personal income, which can reflect the average economic level of residents.\n",
    "\n",
    "3. **Disposable Personal Income**: the remaining income of residents after subtracting necessary taxes and other involuntary expenditures from their income. This indicator can help to understand the real purchasing power of the population.\n",
    "\n",
    "4. **Personal Consumption Expenditures**: indicates how much the population spends on a variety of goods and services, which may be related to the financial activities of consumers and their vulnerability to fraud.\n",
    "\n",
    "5. **Regional Price Parity**: reflects differences in price levels in different regions, which may affect the real purchasing power of the population and the choice of fraudulent strategies.\n",
    "\n",
    "6. **Total Employment**: an indicator of economic activity, where a high employment rate may indicate a stable economy, while a low employment rate may increase the risk of fraud.\n",
    "\n",
    "7. **Education Statistics (higher than the number of people over the age of 25 with a Bachelor's Degree)**: differences in education levels across this region may respond differently to versus receiving a scam call.\n",
    "\n",
    "Dependent variable selection:\n",
    "\n",
    " **Topic of Fraudulent Call**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Importing and cleaning of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing state-by-state economic data for 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns: ['State', 'Disposable personal income', 'Gross domestic product (GDP)', 'Implicit regional price deflator 10', 'Per capita disposable personal income 7', 'Per capita personal consumption expenditures (PCE) 8', 'Per capita personal income 6', 'Personal consumption expenditures', 'Personal income', 'Real GDP (millions of chained 2017 dollars) 1', 'Real PCE (millions of constant (2017) dollars) 3', 'Real per capita PCE 5', 'Real per capita personal income 4', 'Real personal income (millions of constant (2017) dollars) 2', 'Regional price parities (RPPs) 9', 'Total employment (number of jobs)']\n",
      "        State  Gross domestic product (GDP)  Personal income  \\\n",
      "0     Alabama                      281569.0         258362.2   \n",
      "1      Alaska                       65698.8          50349.7   \n",
      "2     Arizona                      475653.7         430083.5   \n",
      "3    Arkansas                      165989.3         160254.2   \n",
      "4  California                     3641643.4        3006647.3   \n",
      "\n",
      "   Disposable personal income  Personal consumption expenditures  \\\n",
      "0                    229599.5                           215104.6   \n",
      "1                     45685.6                            43412.8   \n",
      "2                    377143.3                           368866.6   \n",
      "3                    142906.9                           128662.4   \n",
      "4                   2464043.4                          2352361.6   \n",
      "\n",
      "   Regional price parities (RPPs) 9  Total employment (number of jobs)  \n",
      "0                            87.776                            2869931  \n",
      "1                           101.989                             457687  \n",
      "2                            99.897                            4287595  \n",
      "3                            86.597                            1755536  \n",
      "4                           112.470                           25300974  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fangguoguo/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "file_path = '/Users/fangguoguo/Desktop/fraud_call_project/Table-4_clean.csv'  \n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Fix column names: remove spaces before and after\n",
    "data.columns = data.columns.str.strip()\n",
    "\n",
    "# List all available economic indicators, making sure the column names match those in dataset\n",
    "print(\"columns:\", data.columns.tolist())\n",
    "\n",
    "# Selection of relevant economic indicators\n",
    "selected_columns = [\n",
    "    'State',\n",
    "    'Gross domestic product (GDP)',\n",
    "    'Personal income',\n",
    "    'Disposable personal income',\n",
    "    'Personal consumption expenditures',\n",
    "    'Regional price parities (RPPs) 9',\n",
    "    'Total employment (number of jobs)'\n",
    "]\n",
    "\n",
    "# Creating a new DataFrame\n",
    "economic_2022_df = data[selected_columns].copy()\n",
    "\n",
    "# Check the new DataFrame\n",
    "print(economic_2022_df.head())\n",
    "\n",
    "# Save new DataFrame to CSV\n",
    "economic_2022_df.to_csv('/Users/fangguoguo/Desktop/fraud_call_project/economic_2022_df.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 52 entries, 0 to 51\n",
      "Data columns (total 7 columns):\n",
      " #   Column                             Non-Null Count  Dtype  \n",
      "---  ------                             --------------  -----  \n",
      " 0   State                              52 non-null     object \n",
      " 1   Gross domestic product (GDP)       52 non-null     float64\n",
      " 2   Personal income                    52 non-null     float64\n",
      " 3   Disposable personal income         52 non-null     float64\n",
      " 4   Personal consumption expenditures  52 non-null     float64\n",
      " 5   Regional price parities (RPPs) 9   52 non-null     float64\n",
      " 6   Total employment (number of jobs)  52 non-null     int64  \n",
      "dtypes: float64(5), int64(1), object(1)\n",
      "memory usage: 3.0+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Gross domestic product (GDP)</th>\n",
       "      <th>Personal income</th>\n",
       "      <th>Disposable personal income</th>\n",
       "      <th>Personal consumption expenditures</th>\n",
       "      <th>Regional price parities (RPPs) 9</th>\n",
       "      <th>Total employment (number of jobs)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>281569.0</td>\n",
       "      <td>258362.2</td>\n",
       "      <td>229599.5</td>\n",
       "      <td>215104.6</td>\n",
       "      <td>87.776</td>\n",
       "      <td>2869931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>65698.8</td>\n",
       "      <td>50349.7</td>\n",
       "      <td>45685.6</td>\n",
       "      <td>43412.8</td>\n",
       "      <td>101.989</td>\n",
       "      <td>457687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>475653.7</td>\n",
       "      <td>430083.5</td>\n",
       "      <td>377143.3</td>\n",
       "      <td>368866.6</td>\n",
       "      <td>99.897</td>\n",
       "      <td>4287595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>165989.3</td>\n",
       "      <td>160254.2</td>\n",
       "      <td>142906.9</td>\n",
       "      <td>128662.4</td>\n",
       "      <td>86.597</td>\n",
       "      <td>1755536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>3641643.4</td>\n",
       "      <td>3006647.3</td>\n",
       "      <td>2464043.4</td>\n",
       "      <td>2352361.6</td>\n",
       "      <td>112.470</td>\n",
       "      <td>25300974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        State  Gross domestic product (GDP)  Personal income  \\\n",
       "0     Alabama                      281569.0         258362.2   \n",
       "1      Alaska                       65698.8          50349.7   \n",
       "2     Arizona                      475653.7         430083.5   \n",
       "3    Arkansas                      165989.3         160254.2   \n",
       "4  California                     3641643.4        3006647.3   \n",
       "\n",
       "   Disposable personal income  Personal consumption expenditures  \\\n",
       "0                    229599.5                           215104.6   \n",
       "1                     45685.6                            43412.8   \n",
       "2                    377143.3                           368866.6   \n",
       "3                    142906.9                           128662.4   \n",
       "4                   2464043.4                          2352361.6   \n",
       "\n",
       "   Regional price parities (RPPs) 9  Total employment (number of jobs)  \n",
       "0                            87.776                            2869931  \n",
       "1                           101.989                             457687  \n",
       "2                            99.897                            4287595  \n",
       "3                            86.597                            1755536  \n",
       "4                           112.470                           25300974  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "economic_2022_df.info()\n",
    "economic_2022_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing education statistics for 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51 entries, 0 to 50\n",
      "Data columns (total 2 columns):\n",
      " #   Column                                             Non-Null Count  Dtype \n",
      "---  ------                                             --------------  ----- \n",
      " 0   State                                              51 non-null     object\n",
      " 1   Number of persons age 25 and over \n",
      "(in thousands)  51 non-null     object\n",
      "dtypes: object(2)\n",
      "memory usage: 948.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "education_data_path = '/Users/fangguoguo/Desktop/fraud_call_project/tabn104.csv'\n",
    "\n",
    "education_2022_df = pd.read_csv(education_data_path)\n",
    "\n",
    "education_2022_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove spaces and line breaks from all column names\n",
    "education_2022_df.columns = education_2022_df.columns.str.replace('\\n', '').str.strip()\n",
    "\n",
    "# Convert column 'Number of persons age 25 and over (in thousands)' in education_df from object to float\n",
    "# Remove commas and convert data types\n",
    "education_2022_df['Number of persons age 25 and over (in thousands)'] = education_2022_df['Number of persons age 25 and over (in thousands)'].str.replace(',', '').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alabama' 'Alaska' 'Arizona' 'Arkansas' 'California' 'Colorado'\n",
      " 'Connecticut' 'Delaware' 'District of Columbia' 'Florida' 'Georgia'\n",
      " 'Hawaii' 'Idaho' 'Illinois' 'Indiana' 'Iowa' 'Kansas' 'Kentucky'\n",
      " 'Louisiana' 'Maine' 'Maryland' 'Massachusetts' 'Michigan' 'Minnesota'\n",
      " 'Mississippi' 'Missouri' 'Montana' 'Nebraska' 'Nevada' 'New Hampshire'\n",
      " 'New Jersey' 'New Mexico' 'New York' 'North Carolina' 'North Dakota'\n",
      " 'Ohio' 'Oklahoma' 'Oregon' 'Pennsylvania' 'Rhode Island' 'South Carolina'\n",
      " 'South Dakota' 'Tennessee' 'Texas' 'United States' 'Utah' 'Vermont'\n",
      " 'Virginia' 'Washington' 'West Virginia' 'Wisconsin' 'Wyoming']\n",
      "['Alabama ' 'Alaska ' 'Arizona ' 'Arkansas ' 'California ' 'Colorado '\n",
      " 'Connecticut ' 'Delaware ' 'District of Columbia ' 'Florida ' 'Georgia '\n",
      " 'Hawaii ' 'Idaho ' 'Illinois ' 'Indiana ' 'Iowa ' 'Kansas ' 'Kentucky '\n",
      " 'Louisiana ' 'Maine ' 'Maryland ' 'Massachusetts ' 'Michigan '\n",
      " 'Minnesota ' 'Mississippi ' 'Missouri ' 'Montana ' 'Nebraska ' 'Nevada '\n",
      " 'New Hampshire ' 'New Jersey ' 'New Mexico ' 'New York '\n",
      " 'North Carolina ' 'North Dakota ' 'Ohio ' 'Oklahoma ' 'Oregon '\n",
      " 'Pennsylvania ' 'Rhode Island ' 'South Carolina ' 'South Dakota '\n",
      " 'Tennessee ' 'Texas ' 'Utah ' 'Vermont ' 'Virginia ' 'Washington '\n",
      " 'West Virginia ' 'Wisconsin ' 'Wyoming ']\n"
     ]
    }
   ],
   "source": [
    "print(economic_2022_df['State'].unique())\n",
    "print(education_2022_df['State'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Gross domestic product (GDP)</th>\n",
       "      <th>Personal income</th>\n",
       "      <th>Disposable personal income</th>\n",
       "      <th>Personal consumption expenditures</th>\n",
       "      <th>Regional price parities (RPPs) 9</th>\n",
       "      <th>Total employment (number of jobs)</th>\n",
       "      <th>Number of persons age 25 and over (in thousands)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>281569.0</td>\n",
       "      <td>258362.2</td>\n",
       "      <td>229599.5</td>\n",
       "      <td>215104.6</td>\n",
       "      <td>87.776</td>\n",
       "      <td>2869931</td>\n",
       "      <td>3475.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>65698.8</td>\n",
       "      <td>50349.7</td>\n",
       "      <td>45685.6</td>\n",
       "      <td>43412.8</td>\n",
       "      <td>101.989</td>\n",
       "      <td>457687</td>\n",
       "      <td>490.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>475653.7</td>\n",
       "      <td>430083.5</td>\n",
       "      <td>377143.3</td>\n",
       "      <td>368866.6</td>\n",
       "      <td>99.897</td>\n",
       "      <td>4287595</td>\n",
       "      <td>5048.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>165989.3</td>\n",
       "      <td>160254.2</td>\n",
       "      <td>142906.9</td>\n",
       "      <td>128662.4</td>\n",
       "      <td>86.597</td>\n",
       "      <td>1755536</td>\n",
       "      <td>2056.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>3641643.4</td>\n",
       "      <td>3006647.3</td>\n",
       "      <td>2464043.4</td>\n",
       "      <td>2352361.6</td>\n",
       "      <td>112.470</td>\n",
       "      <td>25300974</td>\n",
       "      <td>26878.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        State  Gross domestic product (GDP)  Personal income  \\\n",
       "0     Alabama                      281569.0         258362.2   \n",
       "1      Alaska                       65698.8          50349.7   \n",
       "2     Arizona                      475653.7         430083.5   \n",
       "3    Arkansas                      165989.3         160254.2   \n",
       "4  California                     3641643.4        3006647.3   \n",
       "\n",
       "   Disposable personal income  Personal consumption expenditures  \\\n",
       "0                    229599.5                           215104.6   \n",
       "1                     45685.6                            43412.8   \n",
       "2                    377143.3                           368866.6   \n",
       "3                    142906.9                           128662.4   \n",
       "4                   2464043.4                          2352361.6   \n",
       "\n",
       "   Regional price parities (RPPs) 9  Total employment (number of jobs)  \\\n",
       "0                            87.776                            2869931   \n",
       "1                           101.989                             457687   \n",
       "2                            99.897                            4287595   \n",
       "3                            86.597                            1755536   \n",
       "4                           112.470                           25300974   \n",
       "\n",
       "   Number of persons age 25 and over (in thousands)  \n",
       "0                                            3475.0  \n",
       "1                                             490.0  \n",
       "2                                            5048.0  \n",
       "3                                            2056.0  \n",
       "4                                           26878.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove extra spaces from State column values in education_df\n",
    "education_2022_df['State'] = education_2022_df['State'].str.strip()\n",
    "\n",
    "# Re-merge\n",
    "eco_edu_2022_df = pd.merge(economic_2022_df, education_2022_df, on=\"State\")\n",
    "\n",
    "# Print the first few lines of the merged result\n",
    "eco_edu_2022_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing data on economic losses from 2022 fraudulent calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 52 entries, 0 to 51\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   State         52 non-null     object\n",
      " 1   # of Reports  52 non-null     object\n",
      " 2   Total $ Loss  52 non-null     object\n",
      "dtypes: object(3)\n",
      "memory usage: 1.3+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th># of Reports</th>\n",
       "      <th>Total $ Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>22,113</td>\n",
       "      <td>$53,864,805.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>4,409</td>\n",
       "      <td>$16,691,422.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>43,960</td>\n",
       "      <td>$173,944,111.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>12,917</td>\n",
       "      <td>$34,563,485.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>213,223</td>\n",
       "      <td>$1,348,767,079.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        State # of Reports       Total $ Loss\n",
       "0     Alabama       22,113     $53,864,805.00\n",
       "1      Alaska        4,409     $16,691,422.00\n",
       "2     Arizona       43,960    $173,944,111.00\n",
       "3    Arkansas       12,917     $34,563,485.00\n",
       "4  California      213,223  $1,348,767,079.00"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud_report_2022_path = '/Users/fangguoguo/Desktop/fraud_call_project/2022_CSN_State_Fraud_Reports_and_Losses.csv' \n",
    "\n",
    "fraud_report_2022_df = pd.read_csv(fraud_report_2022_path)\n",
    "selected_columns = ['State', '# of Reports', 'Total $ Loss']\n",
    "\n",
    "fraud_report_2022_df = fraud_report_2022_df[selected_columns].copy()\n",
    "\n",
    "fraud_report_2022_df.info()\n",
    "\n",
    "fraud_report_2022_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 52 entries, 0 to 51\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   State         52 non-null     object \n",
      " 1   # of Reports  52 non-null     int64  \n",
      " 2   Total $ Loss  52 non-null     float64\n",
      "dtypes: float64(1), int64(1), object(1)\n",
      "memory usage: 1.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "\n",
    "# First, clean up the Total $ Loss column, removing possible currency symbols and commas\n",
    "fraud_report_2022_df['Total $ Loss'] = fraud_report_2022_df['Total $ Loss'].replace('[\\$,]', '', regex=True).astype(float)\n",
    "\n",
    "# Clean up # of Reports columns, remove commas\n",
    "fraud_report_2022_df['# of Reports'] = fraud_report_2022_df['# of Reports'].replace(',', '', regex=True).astype(int)\n",
    "\n",
    "# Checking changed data types\n",
    "print(fraud_report_2022_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging data\n",
    "eco_edu_fraud_2022_df = pd.merge(eco_edu_2022_df, fraud_report_2022_df, on='State')\n",
    "\n",
    "eco_edu_fraud_2022_df.head(60)\n",
    "\n",
    "eco_edu_fraud_2022_df.to_csv('/Users/fangguoguo/Desktop/fraud_call_project/eco_edu_fraud_2022_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Multiple linear regression modeling to provide a simple understanding of the relationship between the number of reports of fraudulent calls and economic conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:           # of Reports   R-squared:                       0.994\n",
      "Model:                            OLS   Adj. R-squared:                  0.993\n",
      "Method:                 Least Squares   F-statistic:                     1034.\n",
      "Date:                Sun, 23 Jun 2024   Prob (F-statistic):           9.25e-46\n",
      "Time:                        00:36:52   Log-Likelihood:                -481.15\n",
      "No. Observations:                  51   AIC:                             978.3\n",
      "Df Residuals:                      43   BIC:                             993.7\n",
      "Df Model:                           7                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "====================================================================================================================\n",
      "                                                       coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "const                                            -2.154e+04   7554.410     -2.851      0.007   -3.68e+04   -6304.563\n",
      "Gross domestic product (GDP)                        -0.0369      0.011     -3.311      0.002      -0.059      -0.014\n",
      "Personal income                                      0.0666      0.068      0.975      0.335      -0.071       0.204\n",
      "Disposable personal income                          -0.0698      0.061     -1.141      0.260      -0.193       0.054\n",
      "Personal consumption expenditures                    0.0479      0.053      0.902      0.372      -0.059       0.155\n",
      "Regional price parities (RPPs) 9                   220.5964     76.653      2.878      0.006      66.010     375.182\n",
      "Total employment (number of jobs)                    0.0023      0.003      0.826      0.413      -0.003       0.008\n",
      "Number of persons age 25 and over (in thousands)     5.3845      2.302      2.339      0.024       0.742      10.027\n",
      "==============================================================================\n",
      "Omnibus:                        2.045   Durbin-Watson:                   1.943\n",
      "Prob(Omnibus):                  0.360   Jarque-Bera (JB):                1.507\n",
      "Skew:                           0.419   Prob(JB):                        0.471\n",
      "Kurtosis:                       3.076   Cond. No.                     1.06e+08\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.06e+08. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "# Defining the dependent variable\n",
    "y = eco_edu_fraud_2022_df['# of Reports']\n",
    "\n",
    "# Define the independent variables\n",
    "X = eco_edu_fraud_2022_df[['Gross domestic product (GDP)', 'Personal income', 'Disposable personal income', \n",
    "          'Personal consumption expenditures', 'Regional price parities (RPPs) 9', 'Total employment (number of jobs)', \n",
    "          'Number of persons age 25 and over (in thousands)']]\n",
    "\n",
    "# Adding a constant term (intercept) to the model\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fitting multiple linear regression models\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Summary of the output model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this multiple linear regression model, Regional Price Parities (RPPs) and education level are the significant factors affecting the number of fraudulent calls reported, where each unit increase in RPPs is associated with a statistically significant increase in the number of fraudulent calls reported by about 220.6 and each unit increase in the number of persons aged 25 years and above is associated with a statistically significant increase in the number of fraudulent calls reported by about 5.384. While other variables such as GDP, personal income, disposable personal income, and personal consumption expenditures have an effect on the number of fraudulent phone reports, these effects are not statistically significant. Overall, the adjusted R-squared value of the model is 0.993, indicating that the model explains the variation in the number of fraudulent phone reports well and that the model as a whole is statistically significant (p-value < 0.001 for F-statistic). This suggests that fraudulent telephone activity is more closely associated with the economic characteristics and demographics of certain areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Multiple linear regression modeling for a simple understanding of the relationship between fraudulent call losses and economic characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:           Total $ Loss   R-squared:                       0.969\n",
      "Model:                            OLS   Adj. R-squared:                  0.964\n",
      "Method:                 Least Squares   F-statistic:                     191.5\n",
      "Date:                Sun, 23 Jun 2024   Prob (F-statistic):           2.81e-30\n",
      "Time:                        00:36:52   Log-Likelihood:                -961.00\n",
      "No. Observations:                  51   AIC:                             1938.\n",
      "Df Residuals:                      43   BIC:                             1953.\n",
      "Df Model:                           7                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "====================================================================================================================\n",
      "                                                       coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "const                                             1.716e+08   9.21e+07      1.863      0.069   -1.42e+07    3.57e+08\n",
      "Gross domestic product (GDP)                      -114.3797    135.958     -0.841      0.405    -388.566     159.806\n",
      "Personal income                                   -860.1035    832.801     -1.033      0.307   -2539.608     819.401\n",
      "Disposable personal income                        1155.6053    746.634      1.548      0.129    -350.126    2661.337\n",
      "Personal consumption expenditures                 2112.8684    647.175      3.265      0.002     807.715    3418.022\n",
      "Regional price parities (RPPs) 9                  -1.86e+06   9.35e+05     -1.990      0.053   -3.75e+06    2.54e+04\n",
      "Total employment (number of jobs)                  -24.7977     33.634     -0.737      0.465     -92.628      43.032\n",
      "Number of persons age 25 and over (in thousands) -1.074e+05   2.81e+04     -3.824      0.000   -1.64e+05   -5.07e+04\n",
      "==============================================================================\n",
      "Omnibus:                       14.703   Durbin-Watson:                   2.171\n",
      "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               18.213\n",
      "Skew:                          -1.044   Prob(JB):                     0.000111\n",
      "Kurtosis:                       5.052   Cond. No.                     1.06e+08\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.06e+08. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "# Defining the dependent variable\n",
    "y = eco_edu_fraud_2022_df['Total $ Loss']\n",
    "\n",
    "# Defining the independent variables\n",
    "X = eco_edu_fraud_2022_df[['Gross domestic product (GDP)', 'Personal income', 'Disposable personal income', \n",
    "          'Personal consumption expenditures', 'Regional price parities (RPPs) 9', 'Total employment (number of jobs)', \n",
    "          'Number of persons age 25 and over (in thousands)']]\n",
    "\n",
    "# Adding constant term (intercept) to the model\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fitting multiple linear regression models\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Summary of the output model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This multiple linear regression model shows that personal consumption expenditures and the number of education levels significantly affect total losses from fraudulent calls. Specifically, each one-unit increase in personal consumption expenditures significantly increases total fraudulent call losses by approximately $2,113, while each one-thousand-unit increase in the number of individuals 25 years of age and older who have earned a degree significantly decreases total losses by approximately $107,400. The model has an adjusted R-squared value of 0.964, indicating that it explains the variation in loss amounts well. Despite the overall significance of the model, the effects of other factors such as GDP, personal income, disposable personal income, regional price parity, and total employment were not statistically significant, which may imply that fraudulent telephone losses have a weaker direct relationship with these economic indicators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Next modeling regional economic characteristics and phone scamming strategies (scamming themes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing data on the topic of phone scams (theft type) in 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Theft Type</th>\n",
       "      <th>State</th>\n",
       "      <th>Bank Fraud</th>\n",
       "      <th>Credit Card Fraud</th>\n",
       "      <th>Employment or Tax-Related Fraud</th>\n",
       "      <th>Government Documents or Benefits Fraud</th>\n",
       "      <th>Loan or Lease Fraud</th>\n",
       "      <th>Other Identity Theft</th>\n",
       "      <th>Phone or Utilities Fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>1276</td>\n",
       "      <td>8585</td>\n",
       "      <td>917</td>\n",
       "      <td>415</td>\n",
       "      <td>2754</td>\n",
       "      <td>5876</td>\n",
       "      <td>918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>155</td>\n",
       "      <td>249</td>\n",
       "      <td>88</td>\n",
       "      <td>37</td>\n",
       "      <td>74</td>\n",
       "      <td>227</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>2042</td>\n",
       "      <td>7609</td>\n",
       "      <td>2072</td>\n",
       "      <td>749</td>\n",
       "      <td>2665</td>\n",
       "      <td>6111</td>\n",
       "      <td>1682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>619</td>\n",
       "      <td>2091</td>\n",
       "      <td>540</td>\n",
       "      <td>232</td>\n",
       "      <td>714</td>\n",
       "      <td>1587</td>\n",
       "      <td>457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>11166</td>\n",
       "      <td>64878</td>\n",
       "      <td>10156</td>\n",
       "      <td>4790</td>\n",
       "      <td>14977</td>\n",
       "      <td>32952</td>\n",
       "      <td>6850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Theft Type       State  Bank Fraud  Credit Card Fraud  \\\n",
       "0              Alabama        1276               8585   \n",
       "1               Alaska         155                249   \n",
       "2              Arizona        2042               7609   \n",
       "3             Arkansas         619               2091   \n",
       "4           California       11166              64878   \n",
       "\n",
       "Theft Type  Employment or Tax-Related Fraud  \\\n",
       "0                                       917   \n",
       "1                                        88   \n",
       "2                                      2072   \n",
       "3                                       540   \n",
       "4                                     10156   \n",
       "\n",
       "Theft Type  Government Documents or Benefits Fraud  Loan or Lease Fraud  \\\n",
       "0                                              415                 2754   \n",
       "1                                               37                   74   \n",
       "2                                              749                 2665   \n",
       "3                                              232                  714   \n",
       "4                                             4790                14977   \n",
       "\n",
       "Theft Type  Other Identity Theft  Phone or Utilities Fraud  \n",
       "0                           5876                       918  \n",
       "1                            227                        67  \n",
       "2                           6111                      1682  \n",
       "3                           1587                       457  \n",
       "4                          32952                      6850  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading CSV files\n",
    "file_path = '/Users/fangguoguo/Desktop/fraud_call_project/2022_CSN_State_Identity_Theft_Reports.csv' \n",
    "theft_type_2022 = pd.read_csv(file_path)\n",
    "\n",
    "# Clean up the '# of Reports' column, remove commas and convert to integers\n",
    "theft_type_2022['# of Reports'] = theft_type_2022['# of Reports'].str.replace(',', '').astype(int)\n",
    "\n",
    "# Using the pivot_table method to reshape data\n",
    "theft_type_2022 = theft_type_2022.pivot_table(index='State', columns='Theft Type', values='# of Reports', aggfunc='sum')\n",
    "\n",
    "# Re-use index 'State' as a column\n",
    "theft_type_2022.reset_index(inplace=True)\n",
    "\n",
    "# Display of converted data\n",
    "theft_type_2022.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Gross domestic product (GDP)</th>\n",
       "      <th>Personal income</th>\n",
       "      <th>Disposable personal income</th>\n",
       "      <th>Personal consumption expenditures</th>\n",
       "      <th>Regional price parities (RPPs) 9</th>\n",
       "      <th>Total employment (number of jobs)</th>\n",
       "      <th>Number of persons age 25 and over (in thousands)</th>\n",
       "      <th># of Reports</th>\n",
       "      <th>Total $ Loss</th>\n",
       "      <th>Bank Fraud</th>\n",
       "      <th>Credit Card Fraud</th>\n",
       "      <th>Employment or Tax-Related Fraud</th>\n",
       "      <th>Government Documents or Benefits Fraud</th>\n",
       "      <th>Loan or Lease Fraud</th>\n",
       "      <th>Other Identity Theft</th>\n",
       "      <th>Phone or Utilities Fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>281569.00</td>\n",
       "      <td>258362.20</td>\n",
       "      <td>229599.50</td>\n",
       "      <td>215104.60</td>\n",
       "      <td>87.78</td>\n",
       "      <td>2869931</td>\n",
       "      <td>3475.00</td>\n",
       "      <td>22113</td>\n",
       "      <td>53864805.00</td>\n",
       "      <td>1276</td>\n",
       "      <td>8585</td>\n",
       "      <td>917</td>\n",
       "      <td>415</td>\n",
       "      <td>2754</td>\n",
       "      <td>5876</td>\n",
       "      <td>918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>65698.80</td>\n",
       "      <td>50349.70</td>\n",
       "      <td>45685.60</td>\n",
       "      <td>43412.80</td>\n",
       "      <td>101.99</td>\n",
       "      <td>457687</td>\n",
       "      <td>490.00</td>\n",
       "      <td>4409</td>\n",
       "      <td>16691422.00</td>\n",
       "      <td>155</td>\n",
       "      <td>249</td>\n",
       "      <td>88</td>\n",
       "      <td>37</td>\n",
       "      <td>74</td>\n",
       "      <td>227</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>475653.70</td>\n",
       "      <td>430083.50</td>\n",
       "      <td>377143.30</td>\n",
       "      <td>368866.60</td>\n",
       "      <td>99.90</td>\n",
       "      <td>4287595</td>\n",
       "      <td>5048.00</td>\n",
       "      <td>43960</td>\n",
       "      <td>173944111.00</td>\n",
       "      <td>2042</td>\n",
       "      <td>7609</td>\n",
       "      <td>2072</td>\n",
       "      <td>749</td>\n",
       "      <td>2665</td>\n",
       "      <td>6111</td>\n",
       "      <td>1682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>165989.30</td>\n",
       "      <td>160254.20</td>\n",
       "      <td>142906.90</td>\n",
       "      <td>128662.40</td>\n",
       "      <td>86.60</td>\n",
       "      <td>1755536</td>\n",
       "      <td>2056.00</td>\n",
       "      <td>12917</td>\n",
       "      <td>34563485.00</td>\n",
       "      <td>619</td>\n",
       "      <td>2091</td>\n",
       "      <td>540</td>\n",
       "      <td>232</td>\n",
       "      <td>714</td>\n",
       "      <td>1587</td>\n",
       "      <td>457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>3641643.40</td>\n",
       "      <td>3006647.30</td>\n",
       "      <td>2464043.40</td>\n",
       "      <td>2352361.60</td>\n",
       "      <td>112.47</td>\n",
       "      <td>25300974</td>\n",
       "      <td>26878.00</td>\n",
       "      <td>213223</td>\n",
       "      <td>1348767079.00</td>\n",
       "      <td>11166</td>\n",
       "      <td>64878</td>\n",
       "      <td>10156</td>\n",
       "      <td>4790</td>\n",
       "      <td>14977</td>\n",
       "      <td>32952</td>\n",
       "      <td>6850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        State  Gross domestic product (GDP)  Personal income  \\\n",
       "0     Alabama                     281569.00        258362.20   \n",
       "1      Alaska                      65698.80         50349.70   \n",
       "2     Arizona                     475653.70        430083.50   \n",
       "3    Arkansas                     165989.30        160254.20   \n",
       "4  California                    3641643.40       3006647.30   \n",
       "\n",
       "   Disposable personal income  Personal consumption expenditures  \\\n",
       "0                   229599.50                          215104.60   \n",
       "1                    45685.60                           43412.80   \n",
       "2                   377143.30                          368866.60   \n",
       "3                   142906.90                          128662.40   \n",
       "4                  2464043.40                         2352361.60   \n",
       "\n",
       "   Regional price parities (RPPs) 9  Total employment (number of jobs)  \\\n",
       "0                             87.78                            2869931   \n",
       "1                            101.99                             457687   \n",
       "2                             99.90                            4287595   \n",
       "3                             86.60                            1755536   \n",
       "4                            112.47                           25300974   \n",
       "\n",
       "   Number of persons age 25 and over (in thousands)  # of Reports  \\\n",
       "0                                           3475.00         22113   \n",
       "1                                            490.00          4409   \n",
       "2                                           5048.00         43960   \n",
       "3                                           2056.00         12917   \n",
       "4                                          26878.00        213223   \n",
       "\n",
       "   Total $ Loss  Bank Fraud  Credit Card Fraud  \\\n",
       "0   53864805.00        1276               8585   \n",
       "1   16691422.00         155                249   \n",
       "2  173944111.00        2042               7609   \n",
       "3   34563485.00         619               2091   \n",
       "4 1348767079.00       11166              64878   \n",
       "\n",
       "   Employment or Tax-Related Fraud  Government Documents or Benefits Fraud  \\\n",
       "0                              917                                     415   \n",
       "1                               88                                      37   \n",
       "2                             2072                                     749   \n",
       "3                              540                                     232   \n",
       "4                            10156                                    4790   \n",
       "\n",
       "   Loan or Lease Fraud  Other Identity Theft  Phone or Utilities Fraud  \n",
       "0                 2754                  5876                       918  \n",
       "1                   74                   227                        67  \n",
       "2                 2665                  6111                      1682  \n",
       "3                  714                  1587                       457  \n",
       "4                14977                 32952                      6850  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merging data\n",
    "eco_edu_f_t_2022_df = pd.merge(eco_edu_fraud_2022_df, theft_type_2022, on=\"State\")\n",
    "\n",
    "eco_edu_f_t_2022_df.to_csv('/Users/fangguoguo/Desktop/fraud_call_project/eco_edu_f_t_2022_df.csv', index=False)\n",
    "\n",
    "eco_edu_f_t_2022_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every state experiences all types of phone scams, so what we might consider is predicting the severity or frequency of each type of scam, not just its presence or absence.\n",
    "\n",
    "In this case, I plan to treat the data on each fraud topic as continuous variables to be used in the predictive model. For example, the number of scam reports or the economic losses caused by such scams could be used as target variables, rather than simple presence/absence (yes/no) binary labels. So the choice of dependent variable is the number of reports (continuous variable) on the topic of telephone fraud, which includes bank fraud, credit card fraud, employment or tax-related fraud, government document or benefit fraud, loan or lease fraud, other identity theft, and telephone or utility fraud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Building multiple output regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Mean Squared Error (MSE): 10279735.703671264\n",
      "Overall Root Mean Squared Error (RMSE): 3206.2026922313044\n",
      "Overall Mean Absolute Error (MAE): 1504.0388958756762\n",
      "Overall R-squared (R2): 0.6888235549932845\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Preparing the input features (independent variables)\n",
    "X = eco_edu_f_t_2022_df[['Gross domestic product (GDP)', 'Personal income', 'Disposable personal income',\n",
    "          'Personal consumption expenditures', 'Regional price parities (RPPs) 9', 'Total employment (number of jobs)',\n",
    "          'Number of persons age 25 and over (in thousands)']]\n",
    "\n",
    "# Prepare the target variables for multiple scam types\n",
    "y = eco_edu_f_t_2022_df[['Bank Fraud', 'Credit Card Fraud', 'Employment or Tax-Related Fraud', \n",
    "          'Government Documents or Benefits Fraud', 'Loan or Lease Fraud', \n",
    "          'Other Identity Theft', 'Phone or Utilities Fraud']]\n",
    "\n",
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the multioutput linear regression model\n",
    "linear_model = MultiOutputRegressor(LinearRegression())\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = linear_model.predict(X_test)\n",
    "\n",
    "# Calculate overall evaluation metrics\n",
    "linear_overall_mse = mean_squared_error(y_test, y_pred, multioutput='uniform_average')\n",
    "linear_overall_rmse = np.sqrt(linear_overall_mse)\n",
    "linear_overall_mae = mean_absolute_error(y_test, y_pred, multioutput='uniform_average')\n",
    "linear_overall_r2 = r2_score(y_test, y_pred, multioutput='uniform_average')\n",
    "\n",
    "# Print out the overall metrics\n",
    "print(\"Overall Mean Squared Error (MSE):\", linear_overall_mse)\n",
    "print(\"Overall Root Mean Squared Error (RMSE):\", linear_overall_rmse)\n",
    "print(\"Overall Mean Absolute Error (MAE):\", linear_overall_mae)\n",
    "print(\"Overall R-squared (R2):\", linear_overall_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced Multiple Output Linear Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Metric      Train        Test\n",
      "0    MSE 3799680.28 10279735.70\n",
      "1   RMSE    1949.28     3206.20\n",
      "2    MAE    1052.13     1504.04\n",
      "3     R²       0.85        0.69\n",
      "Training time: 0.010259151458740234 seconds\n",
      "Model size: 56 bytes\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Calculate the start time of the model\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the multioutput linear regression model\n",
    "linear_model = MultiOutputRegressor(LinearRegression())\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on both the train and test sets\n",
    "y_train_pred = linear_model.predict(X_train)\n",
    "y_test_pred = linear_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics for training data\n",
    "linear_train_mse = mean_squared_error(y_train, y_train_pred, multioutput='uniform_average')\n",
    "linear_train_rmse = np.sqrt(linear_train_mse)\n",
    "linear_train_mae = mean_absolute_error(y_train, y_train_pred, multioutput='uniform_average')\n",
    "linear_train_r2 = r2_score(y_train, y_train_pred, multioutput='uniform_average')\n",
    "\n",
    "# Calculate metrics for testing data\n",
    "linear_test_mse = mean_squared_error(y_test, y_test_pred, multioutput='uniform_average')\n",
    "linear_test_rmse = np.sqrt(linear_test_mse)\n",
    "linear_test_mae = mean_absolute_error(y_test, y_test_pred, multioutput='uniform_average')\n",
    "linear_test_r2 = r2_score(y_test, y_test_pred, multioutput='uniform_average')\n",
    "\n",
    "# Calculating Runtime\n",
    "linear_end_time = time.time()\n",
    "linear_elapsed_time = linear_end_time - start_time\n",
    "\n",
    "# Display Indicator Tables\n",
    "linear_metrics_df = pd.DataFrame({\n",
    "    \"Metric\": [\"MSE\", \"RMSE\", \"MAE\", \"R²\"],\n",
    "    \"Train\": [linear_train_mse, linear_train_rmse, linear_train_mae, linear_train_r2],\n",
    "    \"Test\": [linear_test_mse, linear_test_rmse, linear_test_mae, linear_test_r2]\n",
    "})\n",
    "\n",
    "print(linear_metrics_df)\n",
    "\n",
    "# Print out the training time of the model\n",
    "print(\"Training time:\", linear_elapsed_time, \"seconds\")\n",
    "\n",
    "# Computational Model Memory Usage\n",
    "print(\"Model size:\", sys.getsizeof(linear_model), \"bytes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multi-output regression model performed reasonably well in predicting different types of phone fraud cases, where the coefficient of determination was 0.6888, showing that the model was able to explain about 68.88% of the data variability. However, the high average prediction error (RMSE of about 3206 cases and MAE of about 1504 cases) points out that the model still has room for improvement. I will choose to use the Lasso model to optimize the multiple output regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Building the Lasso model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² score: 0.7094341992869276\n",
      "Mean Squared Error: 7888587.031392873\n",
      "Root Mean Squared Error: 2452.789819708901\n",
      "Mean Absolute Error: 1408.8104248849302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fangguoguo/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.884e+07, tolerance: 3.011e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/fangguoguo/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.392e+08, tolerance: 7.282e+05\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/fangguoguo/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.512e+04, tolerance: 1.193e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/fangguoguo/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.318e+06, tolerance: 5.966e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/fangguoguo/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.368e+07, tolerance: 5.632e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/fangguoguo/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.622e+08, tolerance: 2.745e+05\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/fangguoguo/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.708e+06, tolerance: 9.404e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/fangguoguo/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Selection of numerical features for normalization\n",
    "features = eco_edu_f_t_2022_df.columns[1:8]  \n",
    "X = eco_edu_f_t_2022_df[features]\n",
    "\n",
    "# Initialize and apply standardizers\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Select fraud-related columns as output variables\n",
    "scam_features = eco_edu_f_t_2022_df.columns[10:]  \n",
    "Y = eco_edu_f_t_2022_df[scam_features]\n",
    "\n",
    "# Split the dataset into training set and test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize Lasso regression model, set more iterations\n",
    "lasso_model = Lasso(alpha=0.1, max_iter=10000)\n",
    "\n",
    "# Training model\n",
    "lasso_model.fit(X_train, Y_train)\n",
    "\n",
    "# Evaluating models using test data\n",
    "lasso_score = lasso_model.score(X_test, Y_test)\n",
    "\n",
    "# Predictions on the test set using the model\n",
    "Y_pred = lasso_model.predict(X_test)\n",
    "\n",
    "# Calculation of assessment indicators\n",
    "r2 = r2_score(Y_test, Y_pred)\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "rmse = mean_squared_error(Y_test, Y_pred, squared=False)\n",
    "mae = mean_absolute_error(Y_test, Y_pred)\n",
    "\n",
    "# Print out the results of the assessment indicators\n",
    "print(f'R² score: {r2}')\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'Root Mean Squared Error: {rmse}')\n",
    "print(f'Mean Absolute Error: {mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the Lasso regression model in predicting the number of phone fraud cases is relatively impressive, successfully explaining about 70.9% of the variability in the data. While this result shows that my model has some predictive power, the average prediction error (RMSE of about 2,452 cases and MAE of about 1,408 cases) suggests that I have room for improvement. To improve the accuracy of my model, I consider adjusting the model parameters to optimize the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimized lasso model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal alpha: 1453.5765059662886\n",
      "R² score: 0.7730662522207071\n",
      "Mean Squared Error: 6813601.013380618\n",
      "Root Mean Squared Error: 2203.7093222165217\n",
      "Mean Absolute Error: 1282.754843596172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fangguoguo/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import MultiTaskLassoCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "\n",
    "# Selection of numerical features for normalization\n",
    "features = eco_edu_f_t_2022_df.columns[1:8] \n",
    "X = eco_edu_f_t_2022_df[features]\n",
    "\n",
    "# Initialize and apply standardizers\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Select fraud-related columns as output variables\n",
    "scam_features = eco_edu_f_t_2022_df.columns[10:]  \n",
    "Y = eco_edu_f_t_2022_df[scam_features]\n",
    "\n",
    "# Split the dataset into training set and test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the MultiTaskLassoCV regression model and use cross-validation to determine the best alpha\n",
    "lasso = MultiTaskLassoCV(cv=5, random_state=42, max_iter=10000)\n",
    "\n",
    "# Training Model\n",
    "lasso.fit(X_train, Y_train)\n",
    "\n",
    "# Predictions on the test set using the model\n",
    "Y_pred = lasso.predict(X_test)\n",
    "\n",
    "# Calculation of assessment indicators\n",
    "r2 = r2_score(Y_test, Y_pred, multioutput='uniform_average')\n",
    "mse = mean_squared_error(Y_test, Y_pred, multioutput='uniform_average')\n",
    "rmse = mean_squared_error(Y_test, Y_pred, squared=False, multioutput='uniform_average')\n",
    "mae = mean_absolute_error(Y_test, Y_pred, multioutput='uniform_average')\n",
    "\n",
    "# Print out the results of the assessment indicators\n",
    "print(f'Optimal alpha: {lasso.alpha_}')\n",
    "print(f'R² score: {r2}')\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'Root Mean Squared Error: {rmse}')\n",
    "print(f'Mean Absolute Error: {mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced lasso modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Metric      Train       Test\n",
      "0    MSE 6792162.99 6813601.01\n",
      "1   RMSE    2606.18    2610.29\n",
      "2    MAE    1180.93    1282.75\n",
      "3     R²       0.78       0.77\n",
      "Training time: 0.4518468379974365 seconds\n",
      "Model size: 56 bytes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import MultiTaskLassoCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Start counting\n",
    "start_time = time.time()\n",
    "\n",
    "# Selection of numerical features for normalization\n",
    "features = eco_edu_f_t_2022_df.columns[1:8] \n",
    "X = eco_edu_f_t_2022_df[features]\n",
    "\n",
    "# Initialize and apply standardizers\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Select fraud-related columns as output variables\n",
    "scam_features = eco_edu_f_t_2022_df.columns[10:]  \n",
    "Y = eco_edu_f_t_2022_df[scam_features]\n",
    "\n",
    "# Split the dataset into training set and test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the MultiTaskLassoCV regression model and use cross-validation to determine the best alpha\n",
    "lasso = MultiTaskLassoCV(cv=5, random_state=42, max_iter=10000)\n",
    "\n",
    "# Training Models\n",
    "lasso.fit(X_train, Y_train)\n",
    "\n",
    "# Predictions using the model on the training and test sets\n",
    "Y_train_pred = lasso.predict(X_train)\n",
    "Y_test_pred = lasso.predict(X_test)\n",
    "\n",
    "# Compute various evaluation metrics for training and test sets\n",
    "lasso_train_r2 = r2_score(Y_train, Y_train_pred, multioutput='uniform_average')\n",
    "lasso_train_mse = mean_squared_error(Y_train, Y_train_pred, multioutput='uniform_average')\n",
    "lasso_train_rmse = np.sqrt(lasso_train_mse)\n",
    "lasso_train_mae = mean_absolute_error(Y_train, Y_train_pred, multioutput='uniform_average')\n",
    "\n",
    "lasso_test_r2 = r2_score(Y_test, Y_test_pred, multioutput='uniform_average')\n",
    "lasso_test_mse = mean_squared_error(Y_test, Y_test_pred, multioutput='uniform_average')\n",
    "lasso_test_rmse = np.sqrt(lasso_test_mse)\n",
    "lasso_test_mae = mean_absolute_error(Y_test, Y_test_pred, multioutput='uniform_average')\n",
    "\n",
    "# Stopping the timer and calculating the running time\n",
    "lasso_end_time = time.time()\n",
    "lasso_elapsed_time = lasso_end_time - start_time\n",
    "\n",
    "# Creating and displaying indicator tables\n",
    "lasso_metrics_df = pd.DataFrame({\n",
    "    'Metric': ['MSE', 'RMSE', 'MAE', 'R²'],\n",
    "    'Train': [lasso_train_mse, lasso_train_rmse, lasso_train_mae, lasso_train_r2],\n",
    "    'Test': [lasso_test_mse, lasso_test_rmse, lasso_test_mae, lasso_test_r2]\n",
    "})\n",
    "\n",
    "print(lasso_metrics_df)\n",
    "\n",
    "# Print out the training time of the model\n",
    "print(\"Training time:\", lasso_elapsed_time, \"seconds\")\n",
    "\n",
    "# Calculate and print the memory size occupied by the model\n",
    "print(\"Model size:\", sys.getsizeof(lasso), \"bytes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Modeling Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² score: 0.7693225349031566\n",
      "Mean Squared Error: 11925503.233315583\n",
      "Root Mean Squared Error: 2824.606827417942\n",
      "Mean Absolute Error: 1680.613636363636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fangguoguo/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Selection of numerical features for normalization\n",
    "features = eco_edu_f_t_2022_df.columns[1:8]  \n",
    "X = eco_edu_f_t_2022_df[features]\n",
    "\n",
    "# Initialize and apply standardizers\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Select fraud-related columns as output variables\n",
    "scam_features = eco_edu_f_t_2022_df.columns[10:]  \n",
    "Y = eco_edu_f_t_2022_df[scam_features]\n",
    "\n",
    "# Split the dataset into training set and test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize a random forest regression model\n",
    "random_forest = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Training Model\n",
    "random_forest.fit(X_train, Y_train)\n",
    "\n",
    "# Predictions on the test set using the model\n",
    "Y_pred = random_forest.predict(X_test)\n",
    "\n",
    "# Calculation of assessment indicators\n",
    "r2 = r2_score(Y_test, Y_pred, multioutput='uniform_average')\n",
    "mse = mean_squared_error(Y_test, Y_pred, multioutput='uniform_average')\n",
    "rmse = mean_squared_error(Y_test, Y_pred, squared=False, multioutput='uniform_average')\n",
    "mae = mean_absolute_error(Y_test, Y_pred, multioutput='uniform_average')\n",
    "\n",
    "# Print out the results of the assessment indicators\n",
    "print(f'R² score: {r2}')\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'Root Mean Squared Error: {rmse}')\n",
    "print(f'Mean Absolute Error: {mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the random forest regression model in predicting the type of phone fraud cases is relatively impressive, successfully explaining about 76.93% of the variability in the data. This result shows that the model has a good predictive ability, but with an average prediction error (RMSE of about 2,824 cases and MAE of about 1,680 cases). I plan to optimize the model by adjusting its parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimized Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fangguoguo/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/fangguoguo/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/fangguoguo/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/fangguoguo/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/fangguoguo/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/fangguoguo/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/fangguoguo/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/fangguoguo/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .....................max_depth=10, n_estimators=100; total time=   0.1s\n",
      "[CV] END .....................max_depth=10, n_estimators=100; total time=   0.1s\n",
      "[CV] END .....................max_depth=10, n_estimators=100; total time=   0.1s\n",
      "[CV] END .....................max_depth=10, n_estimators=100; total time=   0.1s\n",
      "[CV] END .....................max_depth=10, n_estimators=100; total time=   0.1s\n",
      "[CV] END .....................max_depth=10, n_estimators=200; total time=   0.1s\n",
      "[CV] END .....................max_depth=10, n_estimators=200; total time=   0.1s\n",
      "[CV] END .....................max_depth=10, n_estimators=200; total time=   0.1s\n",
      "[CV] END .....................max_depth=10, n_estimators=200; total time=   0.1s\n",
      "[CV] END .....................max_depth=10, n_estimators=200; total time=   0.1s\n",
      "[CV] END .....................max_depth=10, n_estimators=300; total time=   0.1s\n",
      "[CV] END .....................max_depth=10, n_estimators=300; total time=   0.2s\n",
      "[CV] END .....................max_depth=10, n_estimators=300; total time=   0.2s\n",
      "[CV] END .....................max_depth=10, n_estimators=300; total time=   0.2s\n",
      "[CV] END .....................max_depth=10, n_estimators=300; total time=   0.2s\n",
      "[CV] END .....................max_depth=10, n_estimators=400; total time=   0.2s\n",
      "[CV] END .....................max_depth=10, n_estimators=400; total time=   0.2s\n",
      "[CV] END .....................max_depth=10, n_estimators=400; total time=   0.2s\n",
      "[CV] END .....................max_depth=10, n_estimators=400; total time=   0.2s\n",
      "[CV] END .....................max_depth=20, n_estimators=100; total time=   0.1s\n",
      "[CV] END .....................max_depth=10, n_estimators=400; total time=   0.2s\n",
      "[CV] END .....................max_depth=20, n_estimators=100; total time=   0.1s\n",
      "[CV] END .....................max_depth=20, n_estimators=100; total time=   0.1s\n",
      "[CV] END .....................max_depth=10, n_estimators=500; total time=   0.3s\n",
      "[CV] END .....................max_depth=20, n_estimators=100; total time=   0.1s\n",
      "[CV] END .....................max_depth=20, n_estimators=100; total time=   0.1s\n",
      "[CV] END .....................max_depth=10, n_estimators=500; total time=   0.3s\n",
      "[CV] END .....................max_depth=10, n_estimators=500; total time=   0.3s\n",
      "[CV] END .....................max_depth=20, n_estimators=200; total time=   0.1s\n",
      "[CV] END .....................max_depth=10, n_estimators=500; total time=   0.2s\n",
      "[CV] END .....................max_depth=20, n_estimators=200; total time=   0.1s\n",
      "[CV] END .....................max_depth=20, n_estimators=200; total time=   0.1s\n",
      "[CV] END .....................max_depth=20, n_estimators=200; total time=   0.1s\n",
      "[CV] END .....................max_depth=20, n_estimators=200; total time=   0.1s\n",
      "[CV] END .....................max_depth=10, n_estimators=500; total time=   0.3s\n",
      "[CV] END .....................max_depth=20, n_estimators=300; total time=   0.2s\n",
      "[CV] END .....................max_depth=20, n_estimators=300; total time=   0.2s\n",
      "[CV] END .....................max_depth=20, n_estimators=300; total time=   0.2s\n",
      "[CV] END .....................max_depth=20, n_estimators=300; total time=   0.2s\n",
      "[CV] END .....................max_depth=20, n_estimators=300; total time=   0.2s\n",
      "[CV] END .....................max_depth=20, n_estimators=400; total time=   0.2s\n",
      "[CV] END .....................max_depth=20, n_estimators=400; total time=   0.2s\n",
      "[CV] END .....................max_depth=20, n_estimators=400; total time=   0.2s\n",
      "[CV] END .....................max_depth=20, n_estimators=400; total time=   0.2s\n",
      "[CV] END .....................max_depth=30, n_estimators=100; total time=   0.1s\n",
      "[CV] END .....................max_depth=20, n_estimators=400; total time=   0.2s\n",
      "[CV] END .....................max_depth=30, n_estimators=100; total time=   0.0s\n",
      "[CV] END .....................max_depth=30, n_estimators=100; total time=   0.1s\n",
      "[CV] END .....................max_depth=30, n_estimators=100; total time=   0.1s\n",
      "[CV] END .....................max_depth=30, n_estimators=100; total time=   0.1s\n",
      "[CV] END .....................max_depth=20, n_estimators=500; total time=   0.3s\n",
      "[CV] END .....................max_depth=20, n_estimators=500; total time=   0.3s\n",
      "[CV] END .....................max_depth=20, n_estimators=500; total time=   0.3s\n",
      "[CV] END .....................max_depth=30, n_estimators=200; total time=   0.1s\n",
      "[CV] END .....................max_depth=30, n_estimators=200; total time=   0.1s\n",
      "[CV] END .....................max_depth=20, n_estimators=500; total time=   0.3s\n",
      "[CV] END .....................max_depth=20, n_estimators=500; total time=   0.3s\n",
      "[CV] END .....................max_depth=30, n_estimators=200; total time=   0.1s\n",
      "[CV] END .....................max_depth=30, n_estimators=200; total time=   0.1s\n",
      "[CV] END .....................max_depth=30, n_estimators=200; total time=   0.1s\n",
      "[CV] END .....................max_depth=30, n_estimators=300; total time=   0.2s\n",
      "[CV] END .....................max_depth=30, n_estimators=300; total time=   0.2s\n",
      "[CV] END .....................max_depth=30, n_estimators=300; total time=   0.2s\n",
      "[CV] END .....................max_depth=30, n_estimators=300; total time=   0.2s\n",
      "[CV] END .....................max_depth=30, n_estimators=300; total time=   0.2s\n",
      "[CV] END .....................max_depth=30, n_estimators=400; total time=   0.2s\n",
      "[CV] END .....................max_depth=30, n_estimators=400; total time=   0.2s\n",
      "[CV] END .....................max_depth=30, n_estimators=400; total time=   0.3s\n",
      "[CV] END .....................max_depth=30, n_estimators=400; total time=   0.2s\n",
      "[CV] END .....................max_depth=40, n_estimators=100; total time=   0.1s\n",
      "[CV] END .....................max_depth=40, n_estimators=100; total time=   0.1s\n",
      "[CV] END .....................max_depth=30, n_estimators=400; total time=   0.2s\n",
      "[CV] END .....................max_depth=40, n_estimators=100; total time=   0.1s\n",
      "[CV] END .....................max_depth=30, n_estimators=500; total time=   0.3s\n",
      "[CV] END .....................max_depth=40, n_estimators=100; total time=   0.1s\n",
      "[CV] END .....................max_depth=30, n_estimators=500; total time=   0.3s\n",
      "[CV] END .....................max_depth=40, n_estimators=100; total time=   0.1s\n",
      "[CV] END .....................max_depth=30, n_estimators=500; total time=   0.3s\n",
      "[CV] END .....................max_depth=40, n_estimators=200; total time=   0.1s\n",
      "[CV] END .....................max_depth=30, n_estimators=500; total time=   0.3s\n",
      "[CV] END .....................max_depth=40, n_estimators=200; total time=   0.1s\n",
      "[CV] END .....................max_depth=30, n_estimators=500; total time=   0.3s\n",
      "[CV] END .....................max_depth=40, n_estimators=200; total time=   0.1s\n",
      "[CV] END .....................max_depth=40, n_estimators=200; total time=   0.1s\n",
      "[CV] END .....................max_depth=40, n_estimators=200; total time=   0.1s\n",
      "[CV] END .....................max_depth=40, n_estimators=300; total time=   0.2s\n",
      "[CV] END .....................max_depth=40, n_estimators=300; total time=   0.2s\n",
      "[CV] END .....................max_depth=40, n_estimators=300; total time=   0.2s\n",
      "[CV] END .....................max_depth=40, n_estimators=300; total time=   0.2s\n",
      "[CV] END .....................max_depth=40, n_estimators=300; total time=   0.2s\n",
      "[CV] END .....................max_depth=40, n_estimators=400; total time=   0.2s\n",
      "[CV] END .....................max_depth=40, n_estimators=400; total time=   0.3s\n",
      "[CV] END .....................max_depth=40, n_estimators=400; total time=   0.3s\n",
      "[CV] END .....................max_depth=40, n_estimators=400; total time=   0.2s\n",
      "[CV] END .....................max_depth=50, n_estimators=100; total time=   0.1s\n",
      "[CV] END .....................max_depth=50, n_estimators=100; total time=   0.1s\n",
      "[CV] END .....................max_depth=40, n_estimators=400; total time=   0.3s\n",
      "[CV] END .....................max_depth=50, n_estimators=100; total time=   0.1s\n",
      "[CV] END .....................max_depth=40, n_estimators=500; total time=   0.3s\n",
      "[CV] END .....................max_depth=50, n_estimators=100; total time=   0.1s\n",
      "[CV] END .....................max_depth=50, n_estimators=100; total time=   0.1s\n",
      "[CV] END .....................max_depth=40, n_estimators=500; total time=   0.3s\n",
      "[CV] END .....................max_depth=40, n_estimators=500; total time=   0.3s\n",
      "[CV] END .....................max_depth=40, n_estimators=500; total time=   0.3s\n",
      "[CV] END .....................max_depth=50, n_estimators=200; total time=   0.1s\n",
      "[CV] END .....................max_depth=40, n_estimators=500; total time=   0.3s\n",
      "[CV] END .....................max_depth=50, n_estimators=200; total time=   0.1s\n",
      "[CV] END .....................max_depth=50, n_estimators=200; total time=   0.1s\n",
      "[CV] END .....................max_depth=50, n_estimators=200; total time=   0.1s\n",
      "[CV] END .....................max_depth=50, n_estimators=200; total time=   0.1s\n",
      "[CV] END .....................max_depth=50, n_estimators=300; total time=   0.2s\n",
      "[CV] END .....................max_depth=50, n_estimators=300; total time=   0.2s\n",
      "[CV] END .....................max_depth=50, n_estimators=300; total time=   0.2s\n",
      "[CV] END .....................max_depth=50, n_estimators=300; total time=   0.2s\n",
      "[CV] END .....................max_depth=50, n_estimators=300; total time=   0.2s\n",
      "[CV] END .....................max_depth=50, n_estimators=400; total time=   0.2s\n",
      "[CV] END .....................max_depth=50, n_estimators=400; total time=   0.2s\n",
      "[CV] END .....................max_depth=50, n_estimators=400; total time=   0.2s\n",
      "[CV] END .....................max_depth=50, n_estimators=400; total time=   0.2s\n",
      "[CV] END ...................max_depth=None, n_estimators=100; total time=   0.1s\n",
      "[CV] END .....................max_depth=50, n_estimators=400; total time=   0.2s\n",
      "[CV] END ...................max_depth=None, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...................max_depth=None, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...................max_depth=None, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...................max_depth=None, n_estimators=100; total time=   0.1s\n",
      "[CV] END .....................max_depth=50, n_estimators=500; total time=   0.3s\n",
      "[CV] END .....................max_depth=50, n_estimators=500; total time=   0.3s\n",
      "[CV] END .....................max_depth=50, n_estimators=500; total time=   0.3s\n",
      "[CV] END ...................max_depth=None, n_estimators=200; total time=   0.1s\n",
      "[CV] END ...................max_depth=None, n_estimators=200; total time=   0.1s\n",
      "[CV] END ...................max_depth=None, n_estimators=200; total time=   0.1s\n",
      "[CV] END .....................max_depth=50, n_estimators=500; total time=   0.3s\n",
      "[CV] END .....................max_depth=50, n_estimators=500; total time=   0.3s\n",
      "[CV] END ...................max_depth=None, n_estimators=200; total time=   0.1s\n",
      "[CV] END ...................max_depth=None, n_estimators=200; total time=   0.1s\n",
      "[CV] END ...................max_depth=None, n_estimators=300; total time=   0.2s\n",
      "[CV] END ...................max_depth=None, n_estimators=300; total time=   0.2s\n",
      "[CV] END ...................max_depth=None, n_estimators=300; total time=   0.2s\n",
      "[CV] END ...................max_depth=None, n_estimators=300; total time=   0.2s\n",
      "[CV] END ...................max_depth=None, n_estimators=300; total time=   0.2s\n",
      "[CV] END ...................max_depth=None, n_estimators=400; total time=   0.2s\n",
      "[CV] END ...................max_depth=None, n_estimators=400; total time=   0.2s\n",
      "[CV] END ...................max_depth=None, n_estimators=400; total time=   0.2s\n",
      "[CV] END ...................max_depth=None, n_estimators=400; total time=   0.2s\n",
      "[CV] END ...................max_depth=None, n_estimators=400; total time=   0.2s\n",
      "[CV] END ...................max_depth=None, n_estimators=500; total time=   0.3s\n",
      "[CV] END ...................max_depth=None, n_estimators=500; total time=   0.3s\n",
      "[CV] END ...................max_depth=None, n_estimators=500; total time=   0.3s\n",
      "[CV] END ...................max_depth=None, n_estimators=500; total time=   0.2s\n",
      "[CV] END ...................max_depth=None, n_estimators=500; total time=   0.2s\n",
      "Best parameters: {'max_depth': 10, 'n_estimators': 300}\n",
      "Best cross-validation score: 14429832.07\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Defining the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],  # Number of trees, adjustable and expandable according to needs\n",
    "    'max_depth': [10, 20, 30, 40, 50, None]  # Maximum depth of the tree, None means the tree can grow to any depth\n",
    "}\n",
    "\n",
    "# Creating Random Forest Regression Model\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Setting up grid searches using 50% off cross validation\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', verbose=2, n_jobs=-1)\n",
    "\n",
    "# Training Grid Search Models\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Print the optimal parameters and the corresponding MSE\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(-grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal parameters have been found: max_depth of 10 and n_estimators of 300. I can use these parameters to build an optimized Random Forest regression model and train and evaluate it to confirm the performance of these parameters on real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized R² score: 0.773827697448578\n",
      "Optimized Mean Squared Error: 10743368.627943678\n",
      "Optimized Root Mean Squared Error: 2714.0992173043132\n",
      "Optimized Mean Absolute Error: 1626.6956132756134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fangguoguo/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Initializing Random Forest Regression Models with Optimal Parameters\n",
    "optimized_rf = RandomForestRegressor(n_estimators=300, max_depth=10, random_state=42)\n",
    "\n",
    "# Training model\n",
    "optimized_rf.fit(X_train, Y_train)\n",
    "\n",
    "# Predictions on the test set using the model\n",
    "Y_pred_optimized = optimized_rf.predict(X_test)\n",
    "\n",
    "# Calculation of assessment indicators\n",
    "optimized_r2 = r2_score(Y_test, Y_pred_optimized, multioutput='uniform_average')\n",
    "optimized_mse = mean_squared_error(Y_test, Y_pred_optimized, multioutput='uniform_average')\n",
    "optimized_rmse = mean_squared_error(Y_test, Y_pred_optimized, squared=False, multioutput='uniform_average')\n",
    "optimized_mae = mean_absolute_error(Y_test, Y_pred_optimized, multioutput='uniform_average')\n",
    "\n",
    "# Print out the results of the assessment indicators\n",
    "print(f'Optimized R² score: {optimized_r2}')\n",
    "print(f'Optimized Mean Squared Error: {optimized_mse}')\n",
    "print(f'Optimized Root Mean Squared Error: {optimized_rmse}')\n",
    "print(f'Optimized Mean Absolute Error: {optimized_mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced Random Forest Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Metric      Train        Test\n",
      "0    MSE 2193164.35 10743368.63\n",
      "1   RMSE    1480.93     3277.71\n",
      "2    MAE     536.52     1626.70\n",
      "3     R²       0.94        0.77\n",
      "Training time: 0.12435722351074219 seconds\n",
      "Model size: 56 bytes\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Selection of numerical features for normalization\n",
    "features = eco_edu_f_t_2022_df.columns[1:8]  \n",
    "X = eco_edu_f_t_2022_df[features]\n",
    "\n",
    "# Initialize and apply standardizers\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Select fraud-related columns as output variables\n",
    "scam_features = eco_edu_f_t_2022_df.columns[10:]  \n",
    "Y = eco_edu_f_t_2022_df[scam_features]\n",
    "\n",
    "# Split the dataset into training set and test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initializing Random Forest Regression Models with Optimal Parameters\n",
    "optimized_rf = RandomForestRegressor(n_estimators=300, max_depth=10, random_state=42)\n",
    "\n",
    "# Start counting\n",
    "start_time = time.time()\n",
    "\n",
    "# Training model\n",
    "optimized_rf.fit(X_train, Y_train)\n",
    "\n",
    "# Predictions using the model on the training and test sets\n",
    "Y_train_pred = optimized_rf.predict(X_train)\n",
    "Y_test_pred = optimized_rf.predict(X_test)\n",
    "\n",
    "# Compute various evaluation metrics for training and test sets\n",
    "rf_train_r2 = r2_score(Y_train, Y_train_pred, multioutput='uniform_average')\n",
    "rf_train_mse = mean_squared_error(Y_train, Y_train_pred, multioutput='uniform_average')\n",
    "rf_train_rmse = np.sqrt(rf_train_mse)\n",
    "rf_train_mae = mean_absolute_error(Y_train, Y_train_pred, multioutput='uniform_average')\n",
    "\n",
    "rf_test_r2 = r2_score(Y_test, Y_test_pred, multioutput='uniform_average')\n",
    "rf_test_mse = mean_squared_error(Y_test, Y_test_pred, multioutput='uniform_average')\n",
    "rf_test_rmse = np.sqrt(rf_test_mse)\n",
    "rf_test_mae = mean_absolute_error(Y_test, Y_test_pred, multioutput='uniform_average')\n",
    "\n",
    "# Stopping the timer and calculating the running time\n",
    "rf_end_time = time.time()\n",
    "rf_elapsed_time = rf_end_time - start_time\n",
    "\n",
    "# Creating and displaying indicator tables\n",
    "rf_metrics_df = pd.DataFrame({\n",
    "    'Metric': ['MSE', 'RMSE', 'MAE', 'R²'],\n",
    "    'Train': [rf_train_mse, rf_train_rmse, rf_train_mae, rf_train_r2],\n",
    "    'Test': [rf_test_mse, rf_test_rmse, rf_test_mae, rf_test_r2]\n",
    "})\n",
    "\n",
    "print(rf_metrics_df)\n",
    "\n",
    "# Print out the training time of the model\n",
    "print(\"Training time:\", rf_elapsed_time, \"seconds\")\n",
    "\n",
    "# Calculate and print the memory size occupied by the model\n",
    "print(\"Model size:\", sys.getsizeof(optimized_rf), \"bytes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Building Gradient Boosting Regressor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall GBM R² score: 0.592157731933271\n",
      "Overall GBM Mean Squared Error: 25167487.342226144\n",
      "Overall GBM Root Mean Squared Error: 3896.8444125867754\n",
      "Overall GBM Mean Absolute Error: 2194.9013304380273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fangguoguo/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Selection of numerical features for normalization\n",
    "features = eco_edu_f_t_2022_df.columns[1:8] \n",
    "X = eco_edu_f_t_2022_df[features]\n",
    "\n",
    "# Initialize and apply standardizers\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Select fraud-related columns as output variables\n",
    "scam_features = eco_edu_f_t_2022_df.columns[10:] \n",
    "Y = eco_edu_f_t_2022_df[scam_features]\n",
    "\n",
    "# Split the dataset into training set and test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the gradient boosting regression model\n",
    "gbm = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "# Use MultiOutputRegressor to make GBM support multiple outputs\n",
    "multioutput_gbm = MultiOutputRegressor(gbm)\n",
    "\n",
    "# Training model\n",
    "multioutput_gbm.fit(X_train, Y_train)\n",
    "\n",
    "# Predictions on the test set using the model\n",
    "Y_pred_gbm = multioutput_gbm.predict(X_test)\n",
    "\n",
    "# Calculation of overall assessment indicators\n",
    "gbm_r2 = r2_score(Y_test, Y_pred_gbm, multioutput='uniform_average')\n",
    "gbm_mse = mean_squared_error(Y_test, Y_pred_gbm, multioutput='uniform_average')\n",
    "gbm_rmse = mean_squared_error(Y_test, Y_pred_gbm, squared=False, multioutput='uniform_average')\n",
    "gbm_mae = mean_absolute_error(Y_test, Y_pred_gbm, multioutput='uniform_average')\n",
    "\n",
    "# Print out the results of the assessment indicators\n",
    "print(f'Overall GBM R² score: {gbm_r2}')\n",
    "print(f'Overall GBM Mean Squared Error: {gbm_mse}')\n",
    "print(f'Overall GBM Root Mean Squared Error: {gbm_rmse}')\n",
    "print(f'Overall GBM Mean Absolute Error: {gbm_mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient lifter model performed moderately well in predicting the types of phone fraud cases, where the overall R² score was 0.5922, indicating that the model explained about 59.22% of the variability in the data. However, the model has an overall mean square error (MSE) of 25,167,487.34, a root mean square error (RMSE) of 3896.84, and a mean absolute error (MAE) of 2194.90, which are high error metrics suggesting that there is still much room for improvement in terms of prediction accuracy.\n",
    "\n",
    "However, for the GBM, it is true that it is not possible to directly optimize the multi-output task through the standard GradientBoostingRegressor, as it does not support multiple outputs. This means that standard single-output model optimization techniques (e.g., grid search) cannot be directly applied to process and optimize multiple outputs simultaneously. For the excellence of the model we need to compare at the end, it would not be able to support multiple outputs for accuracy results, so I chose to abandon the optimization of the GBM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced Gradient Boosting Regressor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Metric   Train        Test\n",
      "0    MSE 5395.30 25167487.34\n",
      "1   RMSE   73.45     5016.72\n",
      "2    MAE   41.73     2194.90\n",
      "3     R²    1.00        0.59\n",
      "Training time: 0.11009097099304199 seconds\n",
      "Model size: 56 bytes\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Selection of numerical features for normalization\n",
    "features = eco_edu_f_t_2022_df.columns[1:8] \n",
    "X = eco_edu_f_t_2022_df[features]\n",
    "\n",
    "# Initialize and apply standardizers\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Select fraud-related columns as output variables\n",
    "scam_features = eco_edu_f_t_2022_df.columns[10:]  \n",
    "Y = eco_edu_f_t_2022_df[scam_features]\n",
    "\n",
    "# Split the dataset into training set and test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the gradient boosting regression model\n",
    "gbm = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "# Using the MultiOutputRegressor to make GBM support multiple outputs\n",
    "multioutput_gbm = MultiOutputRegressor(gbm)\n",
    "\n",
    "# Start counting\n",
    "start_time = time.time()\n",
    "\n",
    "# Training model\n",
    "multioutput_gbm.fit(X_train, Y_train)\n",
    "\n",
    "# Predictions using the model on the training and test sets\n",
    "Y_train_pred = multioutput_gbm.predict(X_train)\n",
    "Y_test_pred = multioutput_gbm.predict(X_test)\n",
    "\n",
    "# Compute various evaluation metrics for training and test sets\n",
    "gbm_train_r2 = r2_score(Y_train, Y_train_pred, multioutput='uniform_average')\n",
    "gbm_train_mse = mean_squared_error(Y_train, Y_train_pred, multioutput='uniform_average')\n",
    "gbm_train_rmse = np.sqrt(gbm_train_mse)\n",
    "gbm_train_mae = mean_absolute_error(Y_train, Y_train_pred, multioutput='uniform_average')\n",
    "\n",
    "gbm_test_r2 = r2_score(Y_test, Y_test_pred, multioutput='uniform_average')\n",
    "gbm_test_mse = mean_squared_error(Y_test, Y_test_pred, multioutput='uniform_average')\n",
    "gbm_test_rmse = np.sqrt(gbm_test_mse)\n",
    "gbm_test_mae = mean_absolute_error(Y_test, Y_test_pred, multioutput='uniform_average')\n",
    "\n",
    "# Stopping the timer and calculating the running time\n",
    "gbm_end_time = time.time()\n",
    "gbm_elapsed_time = gbm_end_time - start_time\n",
    "\n",
    "# Creating and displaying indicator tables\n",
    "gbm_metrics_df = pd.DataFrame({\n",
    "    'Metric': ['MSE', 'RMSE', 'MAE', 'R²'],\n",
    "    'Train': [gbm_train_mse, gbm_train_rmse, gbm_train_mae, gbm_train_r2],\n",
    "    'Test': [gbm_test_mse, gbm_test_rmse, gbm_test_mae, gbm_test_r2]\n",
    "})\n",
    "\n",
    "print(gbm_metrics_df)\n",
    "\n",
    "# Print out the training time of the model\n",
    "print(\"Training time:\", gbm_elapsed_time, \"seconds\")\n",
    "\n",
    "# Calculate and print the memory size occupied by the model\n",
    "print(\"Model size:\", sys.getsizeof(multioutput_gbm), \"bytes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Building Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 60843764.0000 - mae: 2838.0928 - val_loss: 22240996.0000 - val_mae: 2980.7031\n",
      "Epoch 2/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 60841440.0000 - mae: 2838.0015 - val_loss: 22240508.0000 - val_mae: 2980.6479\n",
      "Epoch 3/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 60840008.0000 - mae: 2837.9226 - val_loss: 22240004.0000 - val_mae: 2980.5879\n",
      "Epoch 4/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 60837952.0000 - mae: 2837.8306 - val_loss: 22239476.0000 - val_mae: 2980.5210\n",
      "Epoch 5/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 60835992.0000 - mae: 2837.7317 - val_loss: 22238896.0000 - val_mae: 2980.4438\n",
      "Epoch 6/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 60833272.0000 - mae: 2837.6074 - val_loss: 22238272.0000 - val_mae: 2980.3540\n",
      "Epoch 7/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 60831600.0000 - mae: 2837.4844 - val_loss: 22237520.0000 - val_mae: 2980.2483\n",
      "Epoch 8/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 60829924.0000 - mae: 2837.3372 - val_loss: 22236586.0000 - val_mae: 2980.1150\n",
      "Epoch 9/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 60825644.0000 - mae: 2837.1309 - val_loss: 22235594.0000 - val_mae: 2979.9629\n",
      "Epoch 10/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 60822824.0000 - mae: 2836.9170 - val_loss: 22234396.0000 - val_mae: 2979.7837\n",
      "Epoch 11/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 60820344.0000 - mae: 2836.6880 - val_loss: 22233082.0000 - val_mae: 2979.5791\n",
      "Epoch 12/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 60816432.0000 - mae: 2836.4019 - val_loss: 22231642.0000 - val_mae: 2979.3462\n",
      "Epoch 13/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 60811760.0000 - mae: 2836.0645 - val_loss: 22230064.0000 - val_mae: 2979.0806\n",
      "Epoch 14/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 60807748.0000 - mae: 2835.7026 - val_loss: 22228194.0000 - val_mae: 2978.7607\n",
      "Epoch 15/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 60801920.0000 - mae: 2835.2585 - val_loss: 22226144.0000 - val_mae: 2978.4097\n",
      "Epoch 16/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 60797464.0000 - mae: 2834.7935 - val_loss: 22223718.0000 - val_mae: 2977.9951\n",
      "Epoch 17/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 60790672.0000 - mae: 2834.2026 - val_loss: 22220880.0000 - val_mae: 2977.5107\n",
      "Epoch 18/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 60782940.0000 - mae: 2833.5566 - val_loss: 22217664.0000 - val_mae: 2976.9512\n",
      "Epoch 19/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 60776292.0000 - mae: 2832.8289 - val_loss: 22213818.0000 - val_mae: 2976.3022\n",
      "Epoch 20/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 60765592.0000 - mae: 2831.9390 - val_loss: 22209444.0000 - val_mae: 2975.5774\n",
      "Epoch 21/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 60751260.0000 - mae: 2830.8950 - val_loss: 22204346.0000 - val_mae: 2974.7520\n",
      "Epoch 22/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 60739052.0000 - mae: 2829.7524 - val_loss: 22198692.0000 - val_mae: 2973.8125\n",
      "Epoch 23/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 60721836.0000 - mae: 2828.4741 - val_loss: 22192644.0000 - val_mae: 2972.7964\n",
      "Epoch 24/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 60704552.0000 - mae: 2827.0354 - val_loss: 22185776.0000 - val_mae: 2971.6704\n",
      "Epoch 25/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 60688840.0000 - mae: 2825.5530 - val_loss: 22177908.0000 - val_mae: 2970.3794\n",
      "Epoch 26/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 60666060.0000 - mae: 2823.7639 - val_loss: 22169110.0000 - val_mae: 2968.9421\n",
      "Epoch 27/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 60644416.0000 - mae: 2821.8315 - val_loss: 22159354.0000 - val_mae: 2967.3459\n",
      "Epoch 28/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 60606440.0000 - mae: 2819.3955 - val_loss: 22149072.0000 - val_mae: 2965.6611\n",
      "Epoch 29/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 60581412.0000 - mae: 2817.1013 - val_loss: 22136300.0000 - val_mae: 2963.7500\n",
      "Epoch 30/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 60546020.0000 - mae: 2814.2832 - val_loss: 22122494.0000 - val_mae: 2961.6345\n",
      "Epoch 31/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 60509968.0000 - mae: 2811.2617 - val_loss: 22107912.0000 - val_mae: 2959.4009\n",
      "Epoch 32/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 60462048.0000 - mae: 2807.9329 - val_loss: 22092480.0000 - val_mae: 2957.0293\n",
      "Epoch 33/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 60405424.0000 - mae: 2804.2380 - val_loss: 22076018.0000 - val_mae: 2954.4431\n",
      "Epoch 34/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 60372080.0000 - mae: 2800.8384 - val_loss: 22055480.0000 - val_mae: 2951.4004\n",
      "Epoch 35/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 60293012.0000 - mae: 2796.2271 - val_loss: 22035714.0000 - val_mae: 2948.3516\n",
      "Epoch 36/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 60241724.0000 - mae: 2792.0393 - val_loss: 22014328.0000 - val_mae: 2945.0469\n",
      "Epoch 37/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 60184252.0000 - mae: 2787.5088 - val_loss: 21991468.0000 - val_mae: 2941.6792\n",
      "Epoch 38/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 60107592.0000 - mae: 2782.5598 - val_loss: 21967968.0000 - val_mae: 2938.3694\n",
      "Epoch 39/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 60042824.0000 - mae: 2777.6587 - val_loss: 21941986.0000 - val_mae: 2935.0498\n",
      "Epoch 40/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 59971044.0000 - mae: 2772.2246 - val_loss: 21913750.0000 - val_mae: 2931.4333\n",
      "Epoch 41/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 59879884.0000 - mae: 2766.4041 - val_loss: 21884326.0000 - val_mae: 2927.6323\n",
      "Epoch 42/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 59797440.0000 - mae: 2760.6418 - val_loss: 21852426.0000 - val_mae: 2923.5303\n",
      "Epoch 43/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 59706416.0000 - mae: 2754.7258 - val_loss: 21819030.0000 - val_mae: 2919.3223\n",
      "Epoch 44/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 59599464.0000 - mae: 2748.2568 - val_loss: 21784006.0000 - val_mae: 2915.0928\n",
      "Epoch 45/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 59478688.0000 - mae: 2741.3350 - val_loss: 21748490.0000 - val_mae: 2910.7932\n",
      "Epoch 46/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 59368676.0000 - mae: 2734.3545 - val_loss: 21710334.0000 - val_mae: 2906.1758\n",
      "Epoch 47/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 59260588.0000 - mae: 2727.2617 - val_loss: 21668530.0000 - val_mae: 2901.4072\n",
      "Epoch 48/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 59146808.0000 - mae: 2720.2705 - val_loss: 21623478.0000 - val_mae: 2896.3887\n",
      "Epoch 49/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 59001104.0000 - mae: 2712.4268 - val_loss: 21576186.0000 - val_mae: 2891.2146\n",
      "Epoch 50/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 58830484.0000 - mae: 2704.3262 - val_loss: 21530214.0000 - val_mae: 2886.4670\n",
      "Epoch 51/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 58698400.0000 - mae: 2697.1318 - val_loss: 21478226.0000 - val_mae: 2881.5068\n",
      "Epoch 52/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 58560060.0000 - mae: 2689.3777 - val_loss: 21417442.0000 - val_mae: 2876.2400\n",
      "Epoch 53/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 58321696.0000 - mae: 2679.1702 - val_loss: 21362050.0000 - val_mae: 2871.3977\n",
      "Epoch 54/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 58161996.0000 - mae: 2670.6045 - val_loss: 21305036.0000 - val_mae: 2866.4019\n",
      "Epoch 55/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 57987244.0000 - mae: 2662.2920 - val_loss: 21244628.0000 - val_mae: 2861.0659\n",
      "Epoch 56/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 57783160.0000 - mae: 2653.4141 - val_loss: 21183610.0000 - val_mae: 2855.6284\n",
      "Epoch 57/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 57638896.0000 - mae: 2645.5303 - val_loss: 21116396.0000 - val_mae: 2849.6396\n",
      "Epoch 58/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 57367424.0000 - mae: 2635.7903 - val_loss: 21052684.0000 - val_mae: 2843.8938\n",
      "Epoch 59/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 57161768.0000 - mae: 2626.8740 - val_loss: 20981564.0000 - val_mae: 2837.5078\n",
      "Epoch 60/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 56913528.0000 - mae: 2616.6003 - val_loss: 20897520.0000 - val_mae: 2830.0061\n",
      "Epoch 61/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 56716872.0000 - mae: 2608.3845 - val_loss: 20805856.0000 - val_mae: 2821.8364\n",
      "Epoch 62/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 56320080.0000 - mae: 2593.9058 - val_loss: 20725340.0000 - val_mae: 2814.5532\n",
      "Epoch 63/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 56076868.0000 - mae: 2583.4526 - val_loss: 20645514.0000 - val_mae: 2807.3027\n",
      "Epoch 64/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 55802868.0000 - mae: 2573.9814 - val_loss: 20566550.0000 - val_mae: 2800.0757\n",
      "Epoch 65/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 55547172.0000 - mae: 2564.6782 - val_loss: 20484402.0000 - val_mae: 2792.5137\n",
      "Epoch 66/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 55302708.0000 - mae: 2555.3909 - val_loss: 20400186.0000 - val_mae: 2784.7544\n",
      "Epoch 67/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 55105264.0000 - mae: 2547.1951 - val_loss: 20313976.0000 - val_mae: 2776.8271\n",
      "Epoch 68/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 54719412.0000 - mae: 2534.2705 - val_loss: 20236668.0000 - val_mae: 2769.5918\n",
      "Epoch 69/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 54493352.0000 - mae: 2526.4084 - val_loss: 20154058.0000 - val_mae: 2761.8535\n",
      "Epoch 70/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 54251216.0000 - mae: 2516.6616 - val_loss: 20068268.0000 - val_mae: 2753.8704\n",
      "Epoch 71/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 54014548.0000 - mae: 2507.4883 - val_loss: 19978196.0000 - val_mae: 2745.7683\n",
      "Epoch 72/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 53710128.0000 - mae: 2497.7942 - val_loss: 19890728.0000 - val_mae: 2737.9912\n",
      "Epoch 73/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 53371312.0000 - mae: 2485.2126 - val_loss: 19808488.0000 - val_mae: 2730.6375\n",
      "Epoch 74/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 53017720.0000 - mae: 2473.2461 - val_loss: 19725462.0000 - val_mae: 2723.1213\n",
      "Epoch 75/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 52745024.0000 - mae: 2462.6821 - val_loss: 19627708.0000 - val_mae: 2714.2891\n",
      "Epoch 76/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 52458388.0000 - mae: 2452.3362 - val_loss: 19511684.0000 - val_mae: 2703.8623\n",
      "Epoch 77/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 51968356.0000 - mae: 2435.4756 - val_loss: 19410600.0000 - val_mae: 2694.6738\n",
      "Epoch 78/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 51704048.0000 - mae: 2423.9541 - val_loss: 19311240.0000 - val_mae: 2685.6582\n",
      "Epoch 79/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 51382872.0000 - mae: 2411.8669 - val_loss: 19208368.0000 - val_mae: 2676.2134\n",
      "Epoch 80/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 51050688.0000 - mae: 2399.7830 - val_loss: 19105520.0000 - val_mae: 2666.7988\n",
      "Epoch 81/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 50606276.0000 - mae: 2385.6919 - val_loss: 19009124.0000 - val_mae: 2657.8618\n",
      "Epoch 82/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 50309916.0000 - mae: 2376.6067 - val_loss: 18910980.0000 - val_mae: 2648.7798\n",
      "Epoch 83/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 50048688.0000 - mae: 2367.7068 - val_loss: 18809440.0000 - val_mae: 2639.4932\n",
      "Epoch 84/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 49692868.0000 - mae: 2354.7690 - val_loss: 18710728.0000 - val_mae: 2630.4358\n",
      "Epoch 85/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 49325876.0000 - mae: 2342.8103 - val_loss: 18609236.0000 - val_mae: 2620.9512\n",
      "Epoch 86/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 48927380.0000 - mae: 2330.8840 - val_loss: 18509192.0000 - val_mae: 2611.5630\n",
      "Epoch 87/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 48597912.0000 - mae: 2320.7417 - val_loss: 18405180.0000 - val_mae: 2601.8691\n",
      "Epoch 88/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 48318952.0000 - mae: 2310.2546 - val_loss: 18296860.0000 - val_mae: 2591.8887\n",
      "Epoch 89/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 47882304.0000 - mae: 2296.1289 - val_loss: 18193858.0000 - val_mae: 2582.3442\n",
      "Epoch 90/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 47561756.0000 - mae: 2284.5947 - val_loss: 18086072.0000 - val_mae: 2572.4263\n",
      "Epoch 91/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 47150952.0000 - mae: 2270.5020 - val_loss: 17978318.0000 - val_mae: 2562.3940\n",
      "Epoch 92/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 46882224.0000 - mae: 2259.9751 - val_loss: 17861628.0000 - val_mae: 2551.9546\n",
      "Epoch 93/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 46400216.0000 - mae: 2245.8179 - val_loss: 17751122.0000 - val_mae: 2542.0359\n",
      "Epoch 94/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 46009560.0000 - mae: 2234.1489 - val_loss: 17649206.0000 - val_mae: 2532.8203\n",
      "Epoch 95/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 45675048.0000 - mae: 2222.7332 - val_loss: 17544578.0000 - val_mae: 2523.4229\n",
      "Epoch 96/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 45359004.0000 - mae: 2211.0449 - val_loss: 17432824.0000 - val_mae: 2513.4944\n",
      "Epoch 97/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 44925744.0000 - mae: 2197.4573 - val_loss: 17324920.0000 - val_mae: 2503.7949\n",
      "Epoch 98/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 44486820.0000 - mae: 2184.0168 - val_loss: 17217984.0000 - val_mae: 2494.1245\n",
      "Epoch 99/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 44177004.0000 - mae: 2171.6658 - val_loss: 17107604.0000 - val_mae: 2484.3442\n",
      "Epoch 100/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 43708540.0000 - mae: 2156.3435 - val_loss: 17003538.0000 - val_mae: 2474.9480\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "R² score: 0.28691412762723\n",
      "Mean Squared Error: 57701461.04676914\n",
      "Root Mean Squared Error: 5635.669208258036\n",
      "Mean Absolute Error: 3417.1312557071833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fangguoguo/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "# Selection of numerical features for normalization\n",
    "features = eco_edu_f_t_2022_df.columns[1:8] \n",
    "X = eco_edu_f_t_2022_df[features]\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Selection of target variables\n",
    "target_features = eco_edu_f_t_2022_df.columns[10:] \n",
    "Y = eco_edu_f_t_2022_df[target_features]\n",
    "\n",
    "# Segmented data sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Building neural network model\n",
    "model = Sequential([\n",
    "    Dense(128, input_dim=X_train.shape[1], activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(Y_train.shape[1], activation='linear')\n",
    "])\n",
    "\n",
    "# Compilation model\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "# Training model\n",
    "history = model.fit(X_train, Y_train, epochs=100, batch_size=10, verbose=1, validation_split=0.2)\n",
    "\n",
    "# Predictions on the test set using the model\n",
    "Y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculation of assessment indicators\n",
    "r2 = r2_score(Y_test, Y_pred)\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "rmse = mean_squared_error(Y_test, Y_pred, squared=False)\n",
    "mae = mean_absolute_error(Y_test, Y_pred)\n",
    "\n",
    "# Print out the results of the assessment indicators\n",
    "print(\"R² score:\", r2)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"Mean Absolute Error:\", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network model performed poorly in predicting the type of phone fraud cases where the R² score was 0.2869 indicating that the model could only explain about 28.69% of the data variability. In addition, the evaluation metrics of the model showed high errors, where the mean square error (MSE) was 57701461.04, the root mean square error (RMSE) was 5635.66, as well as the mean absolute error (MAE) was 3417.13, which were all much higher than expected. This may indicate that the model structure or training process needs further adjustment and optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimized neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 60843512.0000 - mae: 2838.1401 - val_loss: 22241060.0000 - val_mae: 2980.7520 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60842368.0000 - mae: 2838.0928 - val_loss: 22240764.0000 - val_mae: 2980.7207 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60841788.0000 - mae: 2838.0505 - val_loss: 22240460.0000 - val_mae: 2980.6885 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60840728.0000 - mae: 2838.0049 - val_loss: 22240144.0000 - val_mae: 2980.6555 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60839560.0000 - mae: 2837.9565 - val_loss: 22239808.0000 - val_mae: 2980.6206 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60838680.0000 - mae: 2837.9080 - val_loss: 22239442.0000 - val_mae: 2980.5828 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60837472.0000 - mae: 2837.8521 - val_loss: 22239038.0000 - val_mae: 2980.5408 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60836344.0000 - mae: 2837.7944 - val_loss: 22238604.0000 - val_mae: 2980.4949 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60834484.0000 - mae: 2837.7217 - val_loss: 22238142.0000 - val_mae: 2980.4446 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60832932.0000 - mae: 2837.6497 - val_loss: 22237628.0000 - val_mae: 2980.3877 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 60831488.0000 - mae: 2837.5693 - val_loss: 22237050.0000 - val_mae: 2980.3235 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 60829860.0000 - mae: 2837.4824 - val_loss: 22236422.0000 - val_mae: 2980.2524 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 60827584.0000 - mae: 2837.3779 - val_loss: 22235740.0000 - val_mae: 2980.1736 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60825656.0000 - mae: 2837.2690 - val_loss: 22234976.0000 - val_mae: 2980.0850 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60822392.0000 - mae: 2837.1323 - val_loss: 22234180.0000 - val_mae: 2979.9897 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60820104.0000 - mae: 2837.0005 - val_loss: 22233294.0000 - val_mae: 2979.8831 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 60817228.0000 - mae: 2836.8411 - val_loss: 22232300.0000 - val_mae: 2979.7642 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 60814992.0000 - mae: 2836.6831 - val_loss: 22231192.0000 - val_mae: 2979.6309 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 60810384.0000 - mae: 2836.4714 - val_loss: 22230016.0000 - val_mae: 2979.4868 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60806444.0000 - mae: 2836.2622 - val_loss: 22228720.0000 - val_mae: 2979.3271 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60803432.0000 - mae: 2836.0405 - val_loss: 22227280.0000 - val_mae: 2979.1504 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60796992.0000 - mae: 2835.7529 - val_loss: 22225764.0000 - val_mae: 2978.9600 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 60794040.0000 - mae: 2835.5068 - val_loss: 22224080.0000 - val_mae: 2978.7483 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60787896.0000 - mae: 2835.1958 - val_loss: 22222280.0000 - val_mae: 2978.5190 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60780536.0000 - mae: 2834.8289 - val_loss: 22220362.0000 - val_mae: 2978.2715 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60775856.0000 - mae: 2834.5044 - val_loss: 22218206.0000 - val_mae: 2977.9946 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60768880.0000 - mae: 2834.0874 - val_loss: 22215864.0000 - val_mae: 2977.6924 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60759244.0000 - mae: 2833.6147 - val_loss: 22213380.0000 - val_mae: 2977.3682 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 60751088.0000 - mae: 2833.1421 - val_loss: 22210666.0000 - val_mae: 2977.0117 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60745828.0000 - mae: 2832.6672 - val_loss: 22207690.0000 - val_mae: 2976.6221 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 60733200.0000 - mae: 2832.0725 - val_loss: 22204604.0000 - val_mae: 2976.2097 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 60722328.0000 - mae: 2831.4531 - val_loss: 22201270.0000 - val_mae: 2975.7622 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60715848.0000 - mae: 2830.8545 - val_loss: 22197616.0000 - val_mae: 2975.2710 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60700728.0000 - mae: 2830.1006 - val_loss: 22193828.0000 - val_mae: 2974.7520 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60688204.0000 - mae: 2829.3279 - val_loss: 22189704.0000 - val_mae: 2974.1846 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60678672.0000 - mae: 2828.5737 - val_loss: 22185210.0000 - val_mae: 2973.5671 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 60662904.0000 - mae: 2827.6658 - val_loss: 22180500.0000 - val_mae: 2972.9146 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60643556.0000 - mae: 2826.6467 - val_loss: 22175600.0000 - val_mae: 2972.2261 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60627748.0000 - mae: 2825.6484 - val_loss: 22170356.0000 - val_mae: 2971.4905 - lr: 0.0010\n",
      "Epoch 40/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60617612.0000 - mae: 2824.6978 - val_loss: 22164622.0000 - val_mae: 2970.6924 - lr: 0.0010\n",
      "Epoch 41/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 60594448.0000 - mae: 2823.4385 - val_loss: 22158698.0000 - val_mae: 2969.8538 - lr: 0.0010\n",
      "Epoch 42/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60581344.0000 - mae: 2822.3428 - val_loss: 22152304.0000 - val_mae: 2968.9473 - lr: 0.0010\n",
      "Epoch 43/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60561092.0000 - mae: 2821.0322 - val_loss: 22145720.0000 - val_mae: 2968.0010 - lr: 0.0010\n",
      "Epoch 44/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 60538168.0000 - mae: 2819.6338 - val_loss: 22138840.0000 - val_mae: 2966.9990 - lr: 0.0010\n",
      "Epoch 45/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 60519472.0000 - mae: 2818.2341 - val_loss: 22131508.0000 - val_mae: 2965.9297 - lr: 0.0010\n",
      "Epoch 46/200\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 60488856.0000 - mae: 2816.5659 - val_loss: 22124020.0000 - val_mae: 2964.8213 - lr: 0.0010\n",
      "Epoch 47/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 60467692.0000 - mae: 2814.9971 - val_loss: 22115812.0000 - val_mae: 2963.6506 - lr: 0.0010\n",
      "Epoch 48/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 60441672.0000 - mae: 2813.2454 - val_loss: 22107018.0000 - val_mae: 2962.4336 - lr: 0.0010\n",
      "Epoch 49/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60418880.0000 - mae: 2811.4790 - val_loss: 22097566.0000 - val_mae: 2961.1355 - lr: 0.0010\n",
      "Epoch 50/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 60383628.0000 - mae: 2809.4587 - val_loss: 22087740.0000 - val_mae: 2959.7810 - lr: 0.0010\n",
      "Epoch 51/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 60352600.0000 - mae: 2807.3970 - val_loss: 22077234.0000 - val_mae: 2958.3379 - lr: 0.0010\n",
      "Epoch 52/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60319184.0000 - mae: 2805.1387 - val_loss: 22066020.0000 - val_mae: 2956.7944 - lr: 0.0010\n",
      "Epoch 53/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 60278456.0000 - mae: 2802.8430 - val_loss: 22054410.0000 - val_mae: 2955.1812 - lr: 0.0010\n",
      "Epoch 54/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60256328.0000 - mae: 2800.6567 - val_loss: 22041966.0000 - val_mae: 2953.4648 - lr: 0.0010\n",
      "Epoch 55/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 60205564.0000 - mae: 2797.9370 - val_loss: 22029410.0000 - val_mae: 2951.7131 - lr: 0.0010\n",
      "Epoch 56/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 60161148.0000 - mae: 2795.2388 - val_loss: 22016280.0000 - val_mae: 2949.8674 - lr: 0.0010\n",
      "Epoch 57/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 60136912.0000 - mae: 2792.7800 - val_loss: 22002214.0000 - val_mae: 2947.9023 - lr: 0.0010\n",
      "Epoch 58/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60079980.0000 - mae: 2789.7168 - val_loss: 21988004.0000 - val_mae: 2945.8838 - lr: 0.0010\n",
      "Epoch 59/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 60043100.0000 - mae: 2786.9089 - val_loss: 21972862.0000 - val_mae: 2943.7383 - lr: 0.0010\n",
      "Epoch 60/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60001296.0000 - mae: 2783.8767 - val_loss: 21957258.0000 - val_mae: 2941.5181 - lr: 0.0010\n",
      "Epoch 61/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 59951952.0000 - mae: 2780.7476 - val_loss: 21941482.0000 - val_mae: 2939.2793 - lr: 0.0010\n",
      "Epoch 62/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 59903600.0000 - mae: 2777.4575 - val_loss: 21925328.0000 - val_mae: 2937.0649 - lr: 0.0010\n",
      "Epoch 63/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 59840420.0000 - mae: 2773.9106 - val_loss: 21908828.0000 - val_mae: 2934.7783 - lr: 0.0010\n",
      "Epoch 64/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 59805272.0000 - mae: 2770.7981 - val_loss: 21891162.0000 - val_mae: 2932.3594 - lr: 0.0010\n",
      "Epoch 65/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 59750768.0000 - mae: 2767.2581 - val_loss: 21873172.0000 - val_mae: 2929.9209 - lr: 0.0010\n",
      "Epoch 66/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 59695200.0000 - mae: 2763.5967 - val_loss: 21854788.0000 - val_mae: 2927.6497 - lr: 0.0010\n",
      "Epoch 67/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 59633424.0000 - mae: 2759.7378 - val_loss: 21835818.0000 - val_mae: 2925.4656 - lr: 0.0010\n",
      "Epoch 68/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 59575808.0000 - mae: 2756.0298 - val_loss: 21815864.0000 - val_mae: 2923.1760 - lr: 0.0010\n",
      "Epoch 69/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 59514636.0000 - mae: 2752.0342 - val_loss: 21794834.0000 - val_mae: 2920.7668 - lr: 0.0010\n",
      "Epoch 70/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 59440740.0000 - mae: 2747.8281 - val_loss: 21772916.0000 - val_mae: 2918.2578 - lr: 0.0010\n",
      "Epoch 71/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 59371616.0000 - mae: 2743.7283 - val_loss: 21749668.0000 - val_mae: 2915.6067 - lr: 0.0010\n",
      "Epoch 72/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 59319664.0000 - mae: 2739.8032 - val_loss: 21724984.0000 - val_mae: 2912.8013 - lr: 0.0010\n",
      "Epoch 73/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 59214180.0000 - mae: 2734.6357 - val_loss: 21700586.0000 - val_mae: 2909.9976 - lr: 0.0010\n",
      "Epoch 74/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 59144576.0000 - mae: 2730.3750 - val_loss: 21674536.0000 - val_mae: 2907.0264 - lr: 0.0010\n",
      "Epoch 75/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 59054224.0000 - mae: 2725.4980 - val_loss: 21647648.0000 - val_mae: 2904.0037 - lr: 0.0010\n",
      "Epoch 76/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 58977668.0000 - mae: 2721.0422 - val_loss: 21619164.0000 - val_mae: 2901.0105 - lr: 0.0010\n",
      "Epoch 77/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 58911804.0000 - mae: 2716.7490 - val_loss: 21589674.0000 - val_mae: 2897.9167 - lr: 0.0010\n",
      "Epoch 78/200\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 58808288.0000 - mae: 2711.5159 - val_loss: 21560070.0000 - val_mae: 2894.9780 - lr: 0.0010\n",
      "Epoch 79/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 58698600.0000 - mae: 2706.3989 - val_loss: 21529948.0000 - val_mae: 2892.0725 - lr: 0.0010\n",
      "Epoch 80/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 58620748.0000 - mae: 2701.9731 - val_loss: 21498130.0000 - val_mae: 2889.1028 - lr: 0.0010\n",
      "Epoch 81/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 58532024.0000 - mae: 2697.2549 - val_loss: 21465744.0000 - val_mae: 2886.2104 - lr: 0.0010\n",
      "Epoch 82/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 58430792.0000 - mae: 2691.9041 - val_loss: 21433344.0000 - val_mae: 2883.3030 - lr: 0.0010\n",
      "Epoch 83/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 58317128.0000 - mae: 2686.6797 - val_loss: 21400624.0000 - val_mae: 2880.3525 - lr: 0.0010\n",
      "Epoch 84/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 58228284.0000 - mae: 2682.3262 - val_loss: 21366880.0000 - val_mae: 2877.3081 - lr: 0.0010\n",
      "Epoch 85/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 58111860.0000 - mae: 2676.7603 - val_loss: 21332892.0000 - val_mae: 2874.2332 - lr: 0.0010\n",
      "Epoch 86/200\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 58002848.0000 - mae: 2671.8657 - val_loss: 21297798.0000 - val_mae: 2871.0613 - lr: 0.0010\n",
      "Epoch 87/200\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 57908480.0000 - mae: 2667.0701 - val_loss: 21261560.0000 - val_mae: 2867.8979 - lr: 0.0010\n",
      "Epoch 88/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 57779844.0000 - mae: 2662.1069 - val_loss: 21224944.0000 - val_mae: 2864.8218 - lr: 0.0010\n",
      "Epoch 89/200\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 57645536.0000 - mae: 2656.1719 - val_loss: 21187476.0000 - val_mae: 2861.6680 - lr: 0.0010\n",
      "Epoch 90/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 57543948.0000 - mae: 2651.7295 - val_loss: 21147414.0000 - val_mae: 2858.2988 - lr: 0.0010\n",
      "Epoch 91/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 57431996.0000 - mae: 2646.1335 - val_loss: 21106564.0000 - val_mae: 2854.8577 - lr: 0.0010\n",
      "Epoch 92/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 57265272.0000 - mae: 2640.1370 - val_loss: 21065646.0000 - val_mae: 2851.3872 - lr: 0.0010\n",
      "Epoch 93/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 57119988.0000 - mae: 2634.2778 - val_loss: 21023360.0000 - val_mae: 2847.7959 - lr: 0.0010\n",
      "Epoch 94/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 57034628.0000 - mae: 2629.9805 - val_loss: 20978238.0000 - val_mae: 2843.9644 - lr: 0.0010\n",
      "Epoch 95/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 56873004.0000 - mae: 2624.2805 - val_loss: 20933416.0000 - val_mae: 2840.1382 - lr: 0.0010\n",
      "Epoch 96/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 56730244.0000 - mae: 2618.7097 - val_loss: 20887626.0000 - val_mae: 2836.2231 - lr: 0.0010\n",
      "Epoch 97/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 56601012.0000 - mae: 2613.7925 - val_loss: 20840892.0000 - val_mae: 2832.2151 - lr: 0.0010\n",
      "Epoch 98/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 56409152.0000 - mae: 2606.7537 - val_loss: 20794802.0000 - val_mae: 2828.2422 - lr: 0.0010\n",
      "Epoch 99/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 56256672.0000 - mae: 2601.2666 - val_loss: 20746628.0000 - val_mae: 2824.0891 - lr: 0.0010\n",
      "Epoch 100/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 56128552.0000 - mae: 2596.4312 - val_loss: 20696246.0000 - val_mae: 2819.7490 - lr: 0.0010\n",
      "Epoch 101/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 55933880.0000 - mae: 2589.4346 - val_loss: 20645560.0000 - val_mae: 2815.3674 - lr: 0.0010\n",
      "Epoch 102/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 55750736.0000 - mae: 2583.5083 - val_loss: 20593742.0000 - val_mae: 2810.8770 - lr: 0.0010\n",
      "Epoch 103/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 55646476.0000 - mae: 2579.2231 - val_loss: 20538988.0000 - val_mae: 2806.1279 - lr: 0.0010\n",
      "Epoch 104/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 55414728.0000 - mae: 2571.8792 - val_loss: 20485476.0000 - val_mae: 2801.4521 - lr: 0.0010\n",
      "Epoch 105/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 55267632.0000 - mae: 2566.3215 - val_loss: 20429938.0000 - val_mae: 2796.6023 - lr: 0.0010\n",
      "Epoch 106/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 55086200.0000 - mae: 2561.3516 - val_loss: 20373928.0000 - val_mae: 2791.6826 - lr: 0.0010\n",
      "Epoch 107/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 54923548.0000 - mae: 2555.3391 - val_loss: 20317864.0000 - val_mae: 2786.7471 - lr: 0.0010\n",
      "Epoch 108/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 54740304.0000 - mae: 2549.5408 - val_loss: 20262356.0000 - val_mae: 2781.8320 - lr: 0.0010\n",
      "Epoch 109/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 54556180.0000 - mae: 2543.0254 - val_loss: 20207916.0000 - val_mae: 2776.9919 - lr: 0.0010\n",
      "Epoch 110/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 54377772.0000 - mae: 2537.5552 - val_loss: 20153696.0000 - val_mae: 2772.1458 - lr: 0.0010\n",
      "Epoch 111/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 54178936.0000 - mae: 2531.7712 - val_loss: 20099268.0000 - val_mae: 2767.2529 - lr: 0.0010\n",
      "Epoch 112/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 53997728.0000 - mae: 2525.6340 - val_loss: 20043506.0000 - val_mae: 2762.2412 - lr: 0.0010\n",
      "Epoch 113/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 53819008.0000 - mae: 2520.6514 - val_loss: 19986424.0000 - val_mae: 2757.1045 - lr: 0.0010\n",
      "Epoch 114/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 53651964.0000 - mae: 2514.3188 - val_loss: 19928744.0000 - val_mae: 2752.1152 - lr: 0.0010\n",
      "Epoch 115/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 53434248.0000 - mae: 2508.1089 - val_loss: 19871246.0000 - val_mae: 2747.2378 - lr: 0.0010\n",
      "Epoch 116/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 53213404.0000 - mae: 2501.5176 - val_loss: 19812380.0000 - val_mae: 2742.2283 - lr: 0.0010\n",
      "Epoch 117/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 53065596.0000 - mae: 2496.4507 - val_loss: 19750868.0000 - val_mae: 2736.9963 - lr: 0.0010\n",
      "Epoch 118/200\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 52857992.0000 - mae: 2490.2256 - val_loss: 19690082.0000 - val_mae: 2731.7915 - lr: 0.0010\n",
      "Epoch 119/200\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 52624584.0000 - mae: 2482.2603 - val_loss: 19630300.0000 - val_mae: 2726.6428 - lr: 0.0010\n",
      "Epoch 120/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 52418868.0000 - mae: 2476.0547 - val_loss: 19568276.0000 - val_mae: 2721.2954 - lr: 0.0010\n",
      "Epoch 121/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 52239404.0000 - mae: 2469.6150 - val_loss: 19505492.0000 - val_mae: 2715.8647 - lr: 0.0010\n",
      "Epoch 122/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 52020620.0000 - mae: 2462.2241 - val_loss: 19443436.0000 - val_mae: 2710.4763 - lr: 0.0010\n",
      "Epoch 123/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 51781200.0000 - mae: 2454.9419 - val_loss: 19381382.0000 - val_mae: 2705.0698 - lr: 0.0010\n",
      "Epoch 124/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 51567144.0000 - mae: 2447.6787 - val_loss: 19318338.0000 - val_mae: 2699.5745 - lr: 0.0010\n",
      "Epoch 125/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 51377684.0000 - mae: 2441.2324 - val_loss: 19254268.0000 - val_mae: 2693.9797 - lr: 0.0010\n",
      "Epoch 126/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 51155896.0000 - mae: 2433.7644 - val_loss: 19190908.0000 - val_mae: 2688.4209 - lr: 0.0010\n",
      "Epoch 127/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 50938916.0000 - mae: 2426.9006 - val_loss: 19127552.0000 - val_mae: 2682.8384 - lr: 0.0010\n",
      "Epoch 128/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 50721116.0000 - mae: 2419.6521 - val_loss: 19065308.0000 - val_mae: 2677.3333 - lr: 0.0010\n",
      "Epoch 129/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 50475332.0000 - mae: 2411.1465 - val_loss: 19004130.0000 - val_mae: 2671.9170 - lr: 0.0010\n",
      "Epoch 130/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 50200880.0000 - mae: 2401.7075 - val_loss: 18942544.0000 - val_mae: 2666.4512 - lr: 0.0010\n",
      "Epoch 131/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 50068888.0000 - mae: 2396.5146 - val_loss: 18875608.0000 - val_mae: 2660.5425 - lr: 0.0010\n",
      "Epoch 132/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 49832952.0000 - mae: 2387.8337 - val_loss: 18809350.0000 - val_mae: 2654.6707 - lr: 0.0010\n",
      "Epoch 133/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 49530400.0000 - mae: 2377.4238 - val_loss: 18744250.0000 - val_mae: 2648.8604 - lr: 0.0010\n",
      "Epoch 134/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 49329944.0000 - mae: 2371.0894 - val_loss: 18675762.0000 - val_mae: 2642.7637 - lr: 0.0010\n",
      "Epoch 135/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 49126740.0000 - mae: 2364.2546 - val_loss: 18606484.0000 - val_mae: 2636.5842 - lr: 0.0010\n",
      "Epoch 136/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 48876464.0000 - mae: 2356.2976 - val_loss: 18538180.0000 - val_mae: 2630.5049 - lr: 0.0010\n",
      "Epoch 137/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 48628724.0000 - mae: 2347.8740 - val_loss: 18471504.0000 - val_mae: 2624.7217 - lr: 0.0010\n",
      "Epoch 138/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 48330784.0000 - mae: 2338.4055 - val_loss: 18405820.0000 - val_mae: 2618.9719 - lr: 0.0010\n",
      "Epoch 139/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 48160440.0000 - mae: 2332.4890 - val_loss: 18336284.0000 - val_mae: 2612.9185 - lr: 0.0010\n",
      "Epoch 140/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 47847448.0000 - mae: 2323.7480 - val_loss: 18268818.0000 - val_mae: 2607.0002 - lr: 0.0010\n",
      "Epoch 141/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 47599616.0000 - mae: 2314.8767 - val_loss: 18200052.0000 - val_mae: 2600.9858 - lr: 0.0010\n",
      "Epoch 142/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 47373404.0000 - mae: 2306.7690 - val_loss: 18128032.0000 - val_mae: 2594.7305 - lr: 0.0010\n",
      "Epoch 143/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 47148968.0000 - mae: 2298.2622 - val_loss: 18055584.0000 - val_mae: 2588.3994 - lr: 0.0010\n",
      "Epoch 144/200\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 46841096.0000 - mae: 2288.2927 - val_loss: 17984724.0000 - val_mae: 2582.1816 - lr: 0.0010\n",
      "Epoch 145/200\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 46579864.0000 - mae: 2279.0811 - val_loss: 17913220.0000 - val_mae: 2575.9014 - lr: 0.0010\n",
      "Epoch 146/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 46253548.0000 - mae: 2267.9390 - val_loss: 17841180.0000 - val_mae: 2569.5537 - lr: 0.0010\n",
      "Epoch 147/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 45980192.0000 - mae: 2258.4185 - val_loss: 17765482.0000 - val_mae: 2562.9329 - lr: 0.0010\n",
      "Epoch 148/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 45732344.0000 - mae: 2249.0820 - val_loss: 17685282.0000 - val_mae: 2555.9551 - lr: 0.0010\n",
      "Epoch 149/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 45522960.0000 - mae: 2241.2173 - val_loss: 17603678.0000 - val_mae: 2548.8643 - lr: 0.0010\n",
      "Epoch 150/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 45148392.0000 - mae: 2228.9668 - val_loss: 17525606.0000 - val_mae: 2542.0288 - lr: 0.0010\n",
      "Epoch 151/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 44902168.0000 - mae: 2220.5752 - val_loss: 17445820.0000 - val_mae: 2535.0720 - lr: 0.0010\n",
      "Epoch 152/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 44654372.0000 - mae: 2212.3865 - val_loss: 17366244.0000 - val_mae: 2528.1143 - lr: 0.0010\n",
      "Epoch 153/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 44298384.0000 - mae: 2202.5801 - val_loss: 17289992.0000 - val_mae: 2521.3904 - lr: 0.0010\n",
      "Epoch 154/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 44095440.0000 - mae: 2194.5894 - val_loss: 17212514.0000 - val_mae: 2514.6172 - lr: 0.0010\n",
      "Epoch 155/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 43732728.0000 - mae: 2183.3640 - val_loss: 17137194.0000 - val_mae: 2508.0044 - lr: 0.0010\n",
      "Epoch 156/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 43490436.0000 - mae: 2175.1230 - val_loss: 17059212.0000 - val_mae: 2501.2119 - lr: 0.0010\n",
      "Epoch 157/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 43209512.0000 - mae: 2166.3154 - val_loss: 16980668.0000 - val_mae: 2494.3450 - lr: 0.0010\n",
      "Epoch 158/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 42930248.0000 - mae: 2157.8276 - val_loss: 16902580.0000 - val_mae: 2487.4819 - lr: 0.0010\n",
      "Epoch 159/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 42602864.0000 - mae: 2147.2234 - val_loss: 16825302.0000 - val_mae: 2480.6865 - lr: 0.0010\n",
      "Epoch 160/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 42328784.0000 - mae: 2138.3486 - val_loss: 16746785.0000 - val_mae: 2473.8037 - lr: 0.0010\n",
      "Epoch 161/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 42005092.0000 - mae: 2128.1016 - val_loss: 16667728.0000 - val_mae: 2466.9126 - lr: 0.0010\n",
      "Epoch 162/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 41829384.0000 - mae: 2120.1621 - val_loss: 16586010.0000 - val_mae: 2459.8591 - lr: 0.0010\n",
      "Epoch 163/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 41444368.0000 - mae: 2107.7905 - val_loss: 16507375.0000 - val_mae: 2453.0166 - lr: 0.0010\n",
      "Epoch 164/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 41121272.0000 - mae: 2098.7886 - val_loss: 16427928.0000 - val_mae: 2446.1414 - lr: 0.0010\n",
      "Epoch 165/200\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 40858296.0000 - mae: 2090.8772 - val_loss: 16345118.0000 - val_mae: 2439.0208 - lr: 0.0010\n",
      "Epoch 166/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 40557704.0000 - mae: 2082.0996 - val_loss: 16262588.0000 - val_mae: 2431.9355 - lr: 0.0010\n",
      "Epoch 167/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 40345904.0000 - mae: 2075.1790 - val_loss: 16179216.0000 - val_mae: 2424.8120 - lr: 0.0010\n",
      "Epoch 168/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 40033676.0000 - mae: 2065.5217 - val_loss: 16100092.0000 - val_mae: 2418.0161 - lr: 0.0010\n",
      "Epoch 169/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 39743576.0000 - mae: 2056.9036 - val_loss: 16023866.0000 - val_mae: 2411.4209 - lr: 0.0010\n",
      "Epoch 170/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 39359592.0000 - mae: 2047.2415 - val_loss: 15951212.0000 - val_mae: 2405.0691 - lr: 0.0010\n",
      "Epoch 171/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 39203184.0000 - mae: 2041.6536 - val_loss: 15873948.0000 - val_mae: 2398.3894 - lr: 0.0010\n",
      "Epoch 172/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 38844280.0000 - mae: 2033.4281 - val_loss: 15799768.0000 - val_mae: 2391.9136 - lr: 0.0010\n",
      "Epoch 173/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 38581848.0000 - mae: 2026.6914 - val_loss: 15723554.0000 - val_mae: 2385.3306 - lr: 0.0010\n",
      "Epoch 174/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 38329840.0000 - mae: 2018.0046 - val_loss: 15647833.0000 - val_mae: 2378.9082 - lr: 0.0010\n",
      "Epoch 175/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 38088600.0000 - mae: 2010.7024 - val_loss: 15571810.0000 - val_mae: 2372.4053 - lr: 0.0010\n",
      "Epoch 176/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 37768284.0000 - mae: 2002.1316 - val_loss: 15498172.0000 - val_mae: 2366.0332 - lr: 0.0010\n",
      "Epoch 177/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 37491328.0000 - mae: 1994.3086 - val_loss: 15422930.0000 - val_mae: 2359.5066 - lr: 0.0010\n",
      "Epoch 178/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 37222968.0000 - mae: 1986.8152 - val_loss: 15347543.0000 - val_mae: 2352.9355 - lr: 0.0010\n",
      "Epoch 179/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 36993516.0000 - mae: 1979.8765 - val_loss: 15270636.0000 - val_mae: 2346.2095 - lr: 0.0010\n",
      "Epoch 180/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 36659408.0000 - mae: 1971.9026 - val_loss: 15195987.0000 - val_mae: 2339.6375 - lr: 0.0010\n",
      "Epoch 181/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 36408552.0000 - mae: 1964.0095 - val_loss: 15120754.0000 - val_mae: 2333.1548 - lr: 0.0010\n",
      "Epoch 182/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 36183024.0000 - mae: 1957.2113 - val_loss: 15043680.0000 - val_mae: 2326.4546 - lr: 0.0010\n",
      "Epoch 183/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 35845948.0000 - mae: 1948.1097 - val_loss: 14969065.0000 - val_mae: 2319.9180 - lr: 0.0010\n",
      "Epoch 184/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 35594704.0000 - mae: 1939.9219 - val_loss: 14893678.0000 - val_mae: 2313.4568 - lr: 0.0010\n",
      "Epoch 185/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 35367264.0000 - mae: 1932.6373 - val_loss: 14816776.0000 - val_mae: 2306.7925 - lr: 0.0010\n",
      "Epoch 186/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 34989556.0000 - mae: 1922.5840 - val_loss: 14743410.0000 - val_mae: 2300.3672 - lr: 0.0010\n",
      "Epoch 187/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 34728288.0000 - mae: 1916.0295 - val_loss: 14665817.0000 - val_mae: 2293.6636 - lr: 0.0010\n",
      "Epoch 188/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 34491492.0000 - mae: 1910.2484 - val_loss: 14583728.0000 - val_mae: 2286.6616 - lr: 0.0010\n",
      "Epoch 189/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 34195536.0000 - mae: 1902.1284 - val_loss: 14503112.0000 - val_mae: 2279.9045 - lr: 0.0010\n",
      "Epoch 190/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 33978848.0000 - mae: 1894.8854 - val_loss: 14422306.0000 - val_mae: 2273.2295 - lr: 0.0010\n",
      "Epoch 191/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 33619136.0000 - mae: 1886.7711 - val_loss: 14345144.0000 - val_mae: 2266.7534 - lr: 0.0010\n",
      "Epoch 192/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 33410186.0000 - mae: 1879.5171 - val_loss: 14267434.0000 - val_mae: 2260.3306 - lr: 0.0010\n",
      "Epoch 193/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 33126578.0000 - mae: 1871.6802 - val_loss: 14191296.0000 - val_mae: 2253.9272 - lr: 0.0010\n",
      "Epoch 194/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 32813396.0000 - mae: 1863.5031 - val_loss: 14116510.0000 - val_mae: 2247.5498 - lr: 0.0010\n",
      "Epoch 195/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 32594908.0000 - mae: 1856.5127 - val_loss: 14040496.0000 - val_mae: 2241.0276 - lr: 0.0010\n",
      "Epoch 196/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 32329832.0000 - mae: 1848.7889 - val_loss: 13965882.0000 - val_mae: 2234.9817 - lr: 0.0010\n",
      "Epoch 197/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 32059022.0000 - mae: 1840.5488 - val_loss: 13892244.0000 - val_mae: 2228.9941 - lr: 0.0010\n",
      "Epoch 198/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 31778924.0000 - mae: 1832.8552 - val_loss: 13817374.0000 - val_mae: 2222.7678 - lr: 0.0010\n",
      "Epoch 199/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 31509050.0000 - mae: 1826.1880 - val_loss: 13739220.0000 - val_mae: 2216.1667 - lr: 0.0010\n",
      "Epoch 200/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 31209150.0000 - mae: 1817.2576 - val_loss: 13660870.0000 - val_mae: 2209.6626 - lr: 0.0010\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "R² score: 0.5621838780722858\n",
      "Mean Squared Error: 36051381.1350631\n",
      "Root Mean Squared Error: 4382.743474593713\n",
      "Mean Absolute Error: 2599.5643909008472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fangguoguo/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# Building neural network models\n",
    "model = Sequential([\n",
    "    Dense(128, input_dim=X_train.shape[1], activation='relu'),  # input layer\n",
    "    Dense(64, activation='relu'),  # hidden layer\n",
    "    Dense(Y_train.shape[1], activation='linear')  # output layer\n",
    "])\n",
    "\n",
    "# Compilation model\n",
    "model.compile(loss='mse', optimizer=Adam(learning_rate=0.001), metrics=['mae'])\n",
    "\n",
    "# Learning rate decay\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=0.00001, verbose=1)\n",
    "\n",
    "# Train the model, adjust batch size and number of training rounds\n",
    "history = model.fit(X_train, Y_train, epochs=200, batch_size=20, verbose=1, validation_split=0.2, callbacks=[reduce_lr])\n",
    "\n",
    "# Predictions on the test set using the model\n",
    "Y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculation of assessment indicators\n",
    "r2 = r2_score(Y_test, Y_pred)\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "rmse = mean_squared_error(Y_test, Y_pred, squared=False)\n",
    "mae = mean_absolute_error(Y_test, Y_pred)\n",
    "\n",
    "# Print out the results of the assessment indicators\n",
    "print(\"R² score:\", r2)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"Mean Absolute Error:\", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network model through optimization is clearly optimized as there is an improvement in the model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced Neural Network Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 60844092.0000 - mae: 2838.1274 - val_loss: 22241104.0000 - val_mae: 2980.7361 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 60843048.0000 - mae: 2838.0740 - val_loss: 22240824.0000 - val_mae: 2980.7026 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60842060.0000 - mae: 2838.0195 - val_loss: 22240544.0000 - val_mae: 2980.6685 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 60841116.0000 - mae: 2837.9673 - val_loss: 22240256.0000 - val_mae: 2980.6323 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60840160.0000 - mae: 2837.9114 - val_loss: 22239958.0000 - val_mae: 2980.5952 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60839168.0000 - mae: 2837.8516 - val_loss: 22239642.0000 - val_mae: 2980.5552 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 60837700.0000 - mae: 2837.7861 - val_loss: 22239308.0000 - val_mae: 2980.5137 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 60836628.0000 - mae: 2837.7239 - val_loss: 22238948.0000 - val_mae: 2980.4692 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 60835888.0000 - mae: 2837.6633 - val_loss: 22238554.0000 - val_mae: 2980.4209 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60834212.0000 - mae: 2837.5815 - val_loss: 22238124.0000 - val_mae: 2980.3679 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60833192.0000 - mae: 2837.5085 - val_loss: 22237650.0000 - val_mae: 2980.3083 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 60831448.0000 - mae: 2837.4133 - val_loss: 22237130.0000 - val_mae: 2980.2419 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60829932.0000 - mae: 2837.3196 - val_loss: 22236554.0000 - val_mae: 2980.1687 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60828076.0000 - mae: 2837.2100 - val_loss: 22235932.0000 - val_mae: 2980.0884 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60825476.0000 - mae: 2837.0837 - val_loss: 22235274.0000 - val_mae: 2980.0015 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 60823096.0000 - mae: 2836.9487 - val_loss: 22234548.0000 - val_mae: 2979.9058 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60820860.0000 - mae: 2836.8101 - val_loss: 22233736.0000 - val_mae: 2979.8003 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 60818172.0000 - mae: 2836.6497 - val_loss: 22232832.0000 - val_mae: 2979.6829 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 60815136.0000 - mae: 2836.4712 - val_loss: 22231834.0000 - val_mae: 2979.5532 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60812724.0000 - mae: 2836.2888 - val_loss: 22230728.0000 - val_mae: 2979.4092 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 60808728.0000 - mae: 2836.0615 - val_loss: 22229542.0000 - val_mae: 2979.2527 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60804788.0000 - mae: 2835.8286 - val_loss: 22228242.0000 - val_mae: 2979.0806 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 60801016.0000 - mae: 2835.5752 - val_loss: 22226830.0000 - val_mae: 2978.8931 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60795896.0000 - mae: 2835.2937 - val_loss: 22225316.0000 - val_mae: 2978.6887 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60791456.0000 - mae: 2835.0005 - val_loss: 22223676.0000 - val_mae: 2978.4661 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60783988.0000 - mae: 2834.6270 - val_loss: 22221972.0000 - val_mae: 2978.2292 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60780564.0000 - mae: 2834.3198 - val_loss: 22220066.0000 - val_mae: 2977.9663 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 60773428.0000 - mae: 2833.9160 - val_loss: 22218032.0000 - val_mae: 2977.6816 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60765576.0000 - mae: 2833.4700 - val_loss: 22215812.0000 - val_mae: 2977.3704 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 60757900.0000 - mae: 2833.0103 - val_loss: 22213384.0000 - val_mae: 2977.0303 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60748896.0000 - mae: 2832.4907 - val_loss: 22210778.0000 - val_mae: 2976.6641 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60742500.0000 - mae: 2831.9751 - val_loss: 22207892.0000 - val_mae: 2976.2607 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60732440.0000 - mae: 2831.3723 - val_loss: 22204802.0000 - val_mae: 2975.8267 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60722264.0000 - mae: 2830.7383 - val_loss: 22201484.0000 - val_mae: 2975.3604 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 60710776.0000 - mae: 2830.0239 - val_loss: 22197906.0000 - val_mae: 2974.8535 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 60696848.0000 - mae: 2829.2158 - val_loss: 22194088.0000 - val_mae: 2974.3086 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 60686124.0000 - mae: 2828.4500 - val_loss: 22189920.0000 - val_mae: 2973.7153 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 60671820.0000 - mae: 2827.5657 - val_loss: 22185454.0000 - val_mae: 2973.0750 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 60658624.0000 - mae: 2826.6411 - val_loss: 22180694.0000 - val_mae: 2972.3882 - lr: 0.0010\n",
      "Epoch 40/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 60640848.0000 - mae: 2825.5654 - val_loss: 22175696.0000 - val_mae: 2971.6616 - lr: 0.0010\n",
      "Epoch 41/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60621144.0000 - mae: 2824.4419 - val_loss: 22170380.0000 - val_mae: 2970.8875 - lr: 0.0010\n",
      "Epoch 42/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60608280.0000 - mae: 2823.3672 - val_loss: 22164578.0000 - val_mae: 2970.0454 - lr: 0.0010\n",
      "Epoch 43/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60589472.0000 - mae: 2822.1228 - val_loss: 22158538.0000 - val_mae: 2969.1602 - lr: 0.0010\n",
      "Epoch 44/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60569508.0000 - mae: 2820.8235 - val_loss: 22152260.0000 - val_mae: 2968.2297 - lr: 0.0010\n",
      "Epoch 45/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60549116.0000 - mae: 2819.4619 - val_loss: 22145684.0000 - val_mae: 2967.2437 - lr: 0.0010\n",
      "Epoch 46/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60528416.0000 - mae: 2817.9768 - val_loss: 22138808.0000 - val_mae: 2966.2041 - lr: 0.0010\n",
      "Epoch 47/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60497844.0000 - mae: 2816.3569 - val_loss: 22131784.0000 - val_mae: 2965.1667 - lr: 0.0010\n",
      "Epoch 48/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60481588.0000 - mae: 2814.8730 - val_loss: 22123896.0000 - val_mae: 2964.0449 - lr: 0.0010\n",
      "Epoch 49/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60456192.0000 - mae: 2813.0610 - val_loss: 22115476.0000 - val_mae: 2962.8496 - lr: 0.0010\n",
      "Epoch 50/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60424304.0000 - mae: 2811.1470 - val_loss: 22106630.0000 - val_mae: 2961.5977 - lr: 0.0010\n",
      "Epoch 51/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 60392080.0000 - mae: 2809.1838 - val_loss: 22097276.0000 - val_mae: 2960.2781 - lr: 0.0010\n",
      "Epoch 52/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60370036.0000 - mae: 2807.2061 - val_loss: 22087036.0000 - val_mae: 2958.8438 - lr: 0.0010\n",
      "Epoch 53/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 60336296.0000 - mae: 2805.0276 - val_loss: 22076306.0000 - val_mae: 2957.3381 - lr: 0.0010\n",
      "Epoch 54/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60305712.0000 - mae: 2802.8208 - val_loss: 22065116.0000 - val_mae: 2955.7661 - lr: 0.0010\n",
      "Epoch 55/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 60269508.0000 - mae: 2800.4158 - val_loss: 22053638.0000 - val_mae: 2954.1348 - lr: 0.0010\n",
      "Epoch 56/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60232560.0000 - mae: 2797.9858 - val_loss: 22041856.0000 - val_mae: 2952.4458 - lr: 0.0010\n",
      "Epoch 57/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60180984.0000 - mae: 2795.1616 - val_loss: 22029960.0000 - val_mae: 2950.7134 - lr: 0.0010\n",
      "Epoch 58/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60145568.0000 - mae: 2792.6074 - val_loss: 22017024.0000 - val_mae: 2948.8564 - lr: 0.0010\n",
      "Epoch 59/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60117300.0000 - mae: 2790.0593 - val_loss: 22003142.0000 - val_mae: 2946.8799 - lr: 0.0010\n",
      "Epoch 60/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 60072296.0000 - mae: 2787.1714 - val_loss: 21989002.0000 - val_mae: 2944.8428 - lr: 0.0010\n",
      "Epoch 61/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60014904.0000 - mae: 2783.9265 - val_loss: 21974564.0000 - val_mae: 2942.7405 - lr: 0.0010\n",
      "Epoch 62/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 59967832.0000 - mae: 2780.8306 - val_loss: 21959202.0000 - val_mae: 2940.6177 - lr: 0.0010\n",
      "Epoch 63/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 59927508.0000 - mae: 2777.7651 - val_loss: 21942660.0000 - val_mae: 2938.3779 - lr: 0.0010\n",
      "Epoch 64/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 59873840.0000 - mae: 2774.3362 - val_loss: 21925520.0000 - val_mae: 2936.1484 - lr: 0.0010\n",
      "Epoch 65/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 59824032.0000 - mae: 2770.9563 - val_loss: 21907752.0000 - val_mae: 2933.8677 - lr: 0.0010\n",
      "Epoch 66/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 59760920.0000 - mae: 2767.1899 - val_loss: 21889522.0000 - val_mae: 2931.5107 - lr: 0.0010\n",
      "Epoch 67/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 59692216.0000 - mae: 2763.1287 - val_loss: 21870584.0000 - val_mae: 2929.0583 - lr: 0.0010\n",
      "Epoch 68/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 59629144.0000 - mae: 2759.2556 - val_loss: 21850590.0000 - val_mae: 2926.5225 - lr: 0.0010\n",
      "Epoch 69/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 59585036.0000 - mae: 2755.8635 - val_loss: 21829496.0000 - val_mae: 2923.9790 - lr: 0.0010\n",
      "Epoch 70/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 59515724.0000 - mae: 2751.5967 - val_loss: 21808332.0000 - val_mae: 2921.4346 - lr: 0.0010\n",
      "Epoch 71/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 59419436.0000 - mae: 2746.9609 - val_loss: 21787470.0000 - val_mae: 2919.0366 - lr: 0.0010\n",
      "Epoch 72/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 59379500.0000 - mae: 2743.4714 - val_loss: 21764798.0000 - val_mae: 2916.4565 - lr: 0.0010\n",
      "Epoch 73/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 59286052.0000 - mae: 2738.7234 - val_loss: 21741948.0000 - val_mae: 2913.8296 - lr: 0.0010\n",
      "Epoch 74/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 59210640.0000 - mae: 2734.1914 - val_loss: 21717824.0000 - val_mae: 2911.0676 - lr: 0.0010\n",
      "Epoch 75/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 59131780.0000 - mae: 2729.6685 - val_loss: 21692480.0000 - val_mae: 2908.1758 - lr: 0.0010\n",
      "Epoch 76/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 59048860.0000 - mae: 2725.1160 - val_loss: 21665886.0000 - val_mae: 2905.1462 - lr: 0.0010\n",
      "Epoch 77/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 58987276.0000 - mae: 2720.6140 - val_loss: 21638048.0000 - val_mae: 2901.9736 - lr: 0.0010\n",
      "Epoch 78/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 58869612.0000 - mae: 2715.4429 - val_loss: 21610488.0000 - val_mae: 2898.9238 - lr: 0.0010\n",
      "Epoch 79/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 58767904.0000 - mae: 2710.1987 - val_loss: 21582064.0000 - val_mae: 2895.8879 - lr: 0.0010\n",
      "Epoch 80/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 58715360.0000 - mae: 2706.4563 - val_loss: 21551868.0000 - val_mae: 2892.8516 - lr: 0.0010\n",
      "Epoch 81/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 58616028.0000 - mae: 2701.4236 - val_loss: 21521776.0000 - val_mae: 2889.8567 - lr: 0.0010\n",
      "Epoch 82/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 58519616.0000 - mae: 2696.4968 - val_loss: 21491668.0000 - val_mae: 2886.8408 - lr: 0.0010\n",
      "Epoch 83/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 58387676.0000 - mae: 2690.9668 - val_loss: 21462166.0000 - val_mae: 2883.8652 - lr: 0.0010\n",
      "Epoch 84/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 58329680.0000 - mae: 2687.0674 - val_loss: 21430346.0000 - val_mae: 2880.8926 - lr: 0.0010\n",
      "Epoch 85/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 58216196.0000 - mae: 2681.5830 - val_loss: 21398480.0000 - val_mae: 2877.9646 - lr: 0.0010\n",
      "Epoch 86/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 58124144.0000 - mae: 2676.9795 - val_loss: 21365724.0000 - val_mae: 2874.9497 - lr: 0.0010\n",
      "Epoch 87/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 57992636.0000 - mae: 2671.1863 - val_loss: 21333028.0000 - val_mae: 2871.9285 - lr: 0.0010\n",
      "Epoch 88/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 57886548.0000 - mae: 2666.5674 - val_loss: 21298408.0000 - val_mae: 2868.7297 - lr: 0.0010\n",
      "Epoch 89/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 57804744.0000 - mae: 2661.8213 - val_loss: 21261972.0000 - val_mae: 2865.5146 - lr: 0.0010\n",
      "Epoch 90/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 57642232.0000 - mae: 2655.5149 - val_loss: 21226458.0000 - val_mae: 2862.4688 - lr: 0.0010\n",
      "Epoch 91/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 57576496.0000 - mae: 2651.3289 - val_loss: 21188324.0000 - val_mae: 2859.2046 - lr: 0.0010\n",
      "Epoch 92/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 57404584.0000 - mae: 2644.8188 - val_loss: 21151290.0000 - val_mae: 2856.0115 - lr: 0.0010\n",
      "Epoch 93/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 57317480.0000 - mae: 2640.4912 - val_loss: 21111918.0000 - val_mae: 2852.6218 - lr: 0.0010\n",
      "Epoch 94/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 57165888.0000 - mae: 2634.6106 - val_loss: 21072108.0000 - val_mae: 2849.1829 - lr: 0.0010\n",
      "Epoch 95/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 57074016.0000 - mae: 2630.5449 - val_loss: 21030374.0000 - val_mae: 2845.5735 - lr: 0.0010\n",
      "Epoch 96/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 56922744.0000 - mae: 2624.3040 - val_loss: 20989200.0000 - val_mae: 2841.9951 - lr: 0.0010\n",
      "Epoch 97/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 56805812.0000 - mae: 2619.3296 - val_loss: 20947066.0000 - val_mae: 2838.3218 - lr: 0.0010\n",
      "Epoch 98/200\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 56618392.0000 - mae: 2612.6790 - val_loss: 20906024.0000 - val_mae: 2834.7173 - lr: 0.0010\n",
      "Epoch 99/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 56497568.0000 - mae: 2608.1450 - val_loss: 20862268.0000 - val_mae: 2830.8784 - lr: 0.0010\n",
      "Epoch 100/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 56338656.0000 - mae: 2601.7354 - val_loss: 20817558.0000 - val_mae: 2826.9473 - lr: 0.0010\n",
      "Epoch 101/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 56253684.0000 - mae: 2598.0552 - val_loss: 20770498.0000 - val_mae: 2822.8118 - lr: 0.0010\n",
      "Epoch 102/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 56076616.0000 - mae: 2591.3105 - val_loss: 20724260.0000 - val_mae: 2818.7261 - lr: 0.0010\n",
      "Epoch 103/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 55888612.0000 - mae: 2585.0347 - val_loss: 20678184.0000 - val_mae: 2814.6323 - lr: 0.0010\n",
      "Epoch 104/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 55794084.0000 - mae: 2580.7915 - val_loss: 20629706.0000 - val_mae: 2810.3357 - lr: 0.0010\n",
      "Epoch 105/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 55617472.0000 - mae: 2574.7358 - val_loss: 20581932.0000 - val_mae: 2806.0811 - lr: 0.0010\n",
      "Epoch 106/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 55457972.0000 - mae: 2569.1309 - val_loss: 20533416.0000 - val_mae: 2801.7515 - lr: 0.0010\n",
      "Epoch 107/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 55322596.0000 - mae: 2564.5437 - val_loss: 20484180.0000 - val_mae: 2797.3403 - lr: 0.0010\n",
      "Epoch 108/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 55140272.0000 - mae: 2558.3931 - val_loss: 20435706.0000 - val_mae: 2792.9810 - lr: 0.0010\n",
      "Epoch 109/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 55003828.0000 - mae: 2554.0073 - val_loss: 20386484.0000 - val_mae: 2788.5476 - lr: 0.0010\n",
      "Epoch 110/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 54843488.0000 - mae: 2548.6016 - val_loss: 20337676.0000 - val_mae: 2784.1362 - lr: 0.0010\n",
      "Epoch 111/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 54682188.0000 - mae: 2542.9297 - val_loss: 20289552.0000 - val_mae: 2779.7759 - lr: 0.0010\n",
      "Epoch 112/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 54481868.0000 - mae: 2537.1748 - val_loss: 20241620.0000 - val_mae: 2775.4062 - lr: 0.0010\n",
      "Epoch 113/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 54368724.0000 - mae: 2532.3538 - val_loss: 20190748.0000 - val_mae: 2770.7876 - lr: 0.0010\n",
      "Epoch 114/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 54139088.0000 - mae: 2525.6294 - val_loss: 20141130.0000 - val_mae: 2766.4673 - lr: 0.0010\n",
      "Epoch 115/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 53988072.0000 - mae: 2520.3892 - val_loss: 20088156.0000 - val_mae: 2761.9541 - lr: 0.0010\n",
      "Epoch 116/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 53859464.0000 - mae: 2515.4189 - val_loss: 20033084.0000 - val_mae: 2757.2546 - lr: 0.0010\n",
      "Epoch 117/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 53672428.0000 - mae: 2509.7119 - val_loss: 19978316.0000 - val_mae: 2752.5518 - lr: 0.0010\n",
      "Epoch 118/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 53492472.0000 - mae: 2503.4592 - val_loss: 19924220.0000 - val_mae: 2747.8770 - lr: 0.0010\n",
      "Epoch 119/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 53315348.0000 - mae: 2498.5249 - val_loss: 19870610.0000 - val_mae: 2743.2227 - lr: 0.0010\n",
      "Epoch 120/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 53112136.0000 - mae: 2491.8303 - val_loss: 19817534.0000 - val_mae: 2738.6008 - lr: 0.0010\n",
      "Epoch 121/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 52935928.0000 - mae: 2486.3167 - val_loss: 19763224.0000 - val_mae: 2733.8530 - lr: 0.0010\n",
      "Epoch 122/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 52723548.0000 - mae: 2479.6113 - val_loss: 19708182.0000 - val_mae: 2729.0283 - lr: 0.0010\n",
      "Epoch 123/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 52536924.0000 - mae: 2473.1609 - val_loss: 19650798.0000 - val_mae: 2723.9946 - lr: 0.0010\n",
      "Epoch 124/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 52339828.0000 - mae: 2466.5254 - val_loss: 19591320.0000 - val_mae: 2718.7737 - lr: 0.0010\n",
      "Epoch 125/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 52163664.0000 - mae: 2460.8425 - val_loss: 19529444.0000 - val_mae: 2713.3386 - lr: 0.0010\n",
      "Epoch 126/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 51951424.0000 - mae: 2454.1235 - val_loss: 19467514.0000 - val_mae: 2707.8772 - lr: 0.0010\n",
      "Epoch 127/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 51764312.0000 - mae: 2446.6943 - val_loss: 19405896.0000 - val_mae: 2702.4297 - lr: 0.0010\n",
      "Epoch 128/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 51547244.0000 - mae: 2439.9634 - val_loss: 19345328.0000 - val_mae: 2697.0488 - lr: 0.0010\n",
      "Epoch 129/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 51340888.0000 - mae: 2432.2358 - val_loss: 19286108.0000 - val_mae: 2691.7705 - lr: 0.0010\n",
      "Epoch 130/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 51127392.0000 - mae: 2425.0447 - val_loss: 19227192.0000 - val_mae: 2686.4963 - lr: 0.0010\n",
      "Epoch 131/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 50848024.0000 - mae: 2415.3257 - val_loss: 19170356.0000 - val_mae: 2681.3745 - lr: 0.0010\n",
      "Epoch 132/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 50728916.0000 - mae: 2410.4912 - val_loss: 19108034.0000 - val_mae: 2675.7856 - lr: 0.0010\n",
      "Epoch 133/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 50430544.0000 - mae: 2400.4932 - val_loss: 19047956.0000 - val_mae: 2670.3538 - lr: 0.0010\n",
      "Epoch 134/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 50303944.0000 - mae: 2395.7583 - val_loss: 18983764.0000 - val_mae: 2664.5742 - lr: 0.0010\n",
      "Epoch 135/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 50041996.0000 - mae: 2387.0554 - val_loss: 18921632.0000 - val_mae: 2658.9443 - lr: 0.0010\n",
      "Epoch 136/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 49830944.0000 - mae: 2381.1978 - val_loss: 18857924.0000 - val_mae: 2653.1526 - lr: 0.0010\n",
      "Epoch 137/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 49631404.0000 - mae: 2374.3921 - val_loss: 18794568.0000 - val_mae: 2647.3975 - lr: 0.0010\n",
      "Epoch 138/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 49349720.0000 - mae: 2365.2244 - val_loss: 18733358.0000 - val_mae: 2641.8130 - lr: 0.0010\n",
      "Epoch 139/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 49155556.0000 - mae: 2357.8469 - val_loss: 18669508.0000 - val_mae: 2636.0154 - lr: 0.0010\n",
      "Epoch 140/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 48960012.0000 - mae: 2351.4324 - val_loss: 18604920.0000 - val_mae: 2630.2642 - lr: 0.0010\n",
      "Epoch 141/200\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 48729024.0000 - mae: 2344.0068 - val_loss: 18541464.0000 - val_mae: 2624.6526 - lr: 0.0010\n",
      "Epoch 142/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 48437368.0000 - mae: 2334.9692 - val_loss: 18479978.0000 - val_mae: 2619.1790 - lr: 0.0010\n",
      "Epoch 143/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 48274624.0000 - mae: 2328.2441 - val_loss: 18416130.0000 - val_mae: 2613.5173 - lr: 0.0010\n",
      "Epoch 144/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 47957888.0000 - mae: 2318.1484 - val_loss: 18353932.0000 - val_mae: 2607.9673 - lr: 0.0010\n",
      "Epoch 145/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 47787860.0000 - mae: 2311.8672 - val_loss: 18286542.0000 - val_mae: 2602.0181 - lr: 0.0010\n",
      "Epoch 146/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 47573748.0000 - mae: 2303.7068 - val_loss: 18218768.0000 - val_mae: 2596.0156 - lr: 0.0010\n",
      "Epoch 147/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 47298392.0000 - mae: 2294.4988 - val_loss: 18152812.0000 - val_mae: 2590.1414 - lr: 0.0010\n",
      "Epoch 148/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 47031280.0000 - mae: 2285.8423 - val_loss: 18085958.0000 - val_mae: 2584.1729 - lr: 0.0010\n",
      "Epoch 149/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 46785592.0000 - mae: 2277.1826 - val_loss: 18018076.0000 - val_mae: 2578.1338 - lr: 0.0010\n",
      "Epoch 150/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 46569312.0000 - mae: 2269.3438 - val_loss: 17948306.0000 - val_mae: 2571.9546 - lr: 0.0010\n",
      "Epoch 151/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 46315224.0000 - mae: 2259.9993 - val_loss: 17878614.0000 - val_mae: 2565.7515 - lr: 0.0010\n",
      "Epoch 152/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 46051960.0000 - mae: 2250.9121 - val_loss: 17808812.0000 - val_mae: 2559.5215 - lr: 0.0010\n",
      "Epoch 153/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 45764440.0000 - mae: 2242.1499 - val_loss: 17738380.0000 - val_mae: 2553.2002 - lr: 0.0010\n",
      "Epoch 154/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 45470336.0000 - mae: 2232.8379 - val_loss: 17667510.0000 - val_mae: 2546.8423 - lr: 0.0010\n",
      "Epoch 155/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 45210784.0000 - mae: 2224.4131 - val_loss: 17593660.0000 - val_mae: 2540.2554 - lr: 0.0010\n",
      "Epoch 156/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 44933348.0000 - mae: 2215.0930 - val_loss: 17517376.0000 - val_mae: 2533.4983 - lr: 0.0010\n",
      "Epoch 157/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 44764736.0000 - mae: 2207.3323 - val_loss: 17439028.0000 - val_mae: 2526.5884 - lr: 0.0010\n",
      "Epoch 158/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 44398760.0000 - mae: 2196.6436 - val_loss: 17363542.0000 - val_mae: 2519.8779 - lr: 0.0010\n",
      "Epoch 159/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 44160176.0000 - mae: 2188.5190 - val_loss: 17286996.0000 - val_mae: 2513.0933 - lr: 0.0010\n",
      "Epoch 160/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 43852188.0000 - mae: 2178.4802 - val_loss: 17211696.0000 - val_mae: 2506.3728 - lr: 0.0010\n",
      "Epoch 161/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 43655216.0000 - mae: 2171.2219 - val_loss: 17134864.0000 - val_mae: 2499.5571 - lr: 0.0010\n",
      "Epoch 162/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 43305000.0000 - mae: 2160.1870 - val_loss: 17061378.0000 - val_mae: 2492.9900 - lr: 0.0010\n",
      "Epoch 163/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 43003568.0000 - mae: 2150.5051 - val_loss: 16987264.0000 - val_mae: 2486.4062 - lr: 0.0010\n",
      "Epoch 164/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 42766392.0000 - mae: 2141.7549 - val_loss: 16910260.0000 - val_mae: 2479.5955 - lr: 0.0010\n",
      "Epoch 165/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 42458916.0000 - mae: 2131.6040 - val_loss: 16833058.0000 - val_mae: 2472.7942 - lr: 0.0010\n",
      "Epoch 166/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 42212896.0000 - mae: 2122.4949 - val_loss: 16753173.0000 - val_mae: 2465.7957 - lr: 0.0010\n",
      "Epoch 167/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 42006452.0000 - mae: 2114.9343 - val_loss: 16673036.0000 - val_mae: 2458.7944 - lr: 0.0010\n",
      "Epoch 168/200\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 41667188.0000 - mae: 2104.0017 - val_loss: 16596116.0000 - val_mae: 2452.0396 - lr: 0.0010\n",
      "Epoch 169/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 41391792.0000 - mae: 2094.5315 - val_loss: 16519826.0000 - val_mae: 2445.3103 - lr: 0.0010\n",
      "Epoch 170/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 41165148.0000 - mae: 2087.0669 - val_loss: 16444567.0000 - val_mae: 2438.6318 - lr: 0.0010\n",
      "Epoch 171/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 40853624.0000 - mae: 2078.3303 - val_loss: 16372198.0000 - val_mae: 2432.1738 - lr: 0.0010\n",
      "Epoch 172/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 40568632.0000 - mae: 2070.8750 - val_loss: 16300016.0000 - val_mae: 2425.6890 - lr: 0.0010\n",
      "Epoch 173/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 40331984.0000 - mae: 2062.7749 - val_loss: 16226366.0000 - val_mae: 2419.1816 - lr: 0.0010\n",
      "Epoch 174/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 40060788.0000 - mae: 2055.9683 - val_loss: 16152224.0000 - val_mae: 2412.6016 - lr: 0.0010\n",
      "Epoch 175/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 39798328.0000 - mae: 2049.0254 - val_loss: 16076657.0000 - val_mae: 2405.8906 - lr: 0.0010\n",
      "Epoch 176/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 39507776.0000 - mae: 2042.8784 - val_loss: 16001730.0000 - val_mae: 2399.2075 - lr: 0.0010\n",
      "Epoch 177/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 39228592.0000 - mae: 2034.9476 - val_loss: 15926816.0000 - val_mae: 2392.6292 - lr: 0.0010\n",
      "Epoch 178/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 38995852.0000 - mae: 2027.8269 - val_loss: 15849833.0000 - val_mae: 2385.9258 - lr: 0.0010\n",
      "Epoch 179/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 38758768.0000 - mae: 2021.0876 - val_loss: 15772210.0000 - val_mae: 2379.1392 - lr: 0.0010\n",
      "Epoch 180/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 38450712.0000 - mae: 2013.1042 - val_loss: 15696674.0000 - val_mae: 2372.4912 - lr: 0.0010\n",
      "Epoch 181/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 38184296.0000 - mae: 2005.8429 - val_loss: 15621036.0000 - val_mae: 2365.8230 - lr: 0.0010\n",
      "Epoch 182/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 37857152.0000 - mae: 1998.4998 - val_loss: 15546550.0000 - val_mae: 2359.2317 - lr: 0.0010\n",
      "Epoch 183/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 37634008.0000 - mae: 1991.9985 - val_loss: 15468482.0000 - val_mae: 2352.4292 - lr: 0.0010\n",
      "Epoch 184/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 37359444.0000 - mae: 1984.5348 - val_loss: 15390167.0000 - val_mae: 2345.6777 - lr: 0.0010\n",
      "Epoch 185/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 37041040.0000 - mae: 1976.0037 - val_loss: 15312614.0000 - val_mae: 2339.0845 - lr: 0.0010\n",
      "Epoch 186/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 36796184.0000 - mae: 1968.9552 - val_loss: 15232694.0000 - val_mae: 2332.3196 - lr: 0.0010\n",
      "Epoch 187/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 36544140.0000 - mae: 1961.1288 - val_loss: 15152046.0000 - val_mae: 2325.5601 - lr: 0.0010\n",
      "Epoch 188/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 36248012.0000 - mae: 1952.8010 - val_loss: 15073140.0000 - val_mae: 2318.8691 - lr: 0.0010\n",
      "Epoch 189/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 36018308.0000 - mae: 1945.3855 - val_loss: 14994278.0000 - val_mae: 2312.0981 - lr: 0.0010\n",
      "Epoch 190/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 35727540.0000 - mae: 1937.2092 - val_loss: 14919445.0000 - val_mae: 2305.6260 - lr: 0.0010\n",
      "Epoch 191/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 35415336.0000 - mae: 1929.7938 - val_loss: 14846489.0000 - val_mae: 2299.2607 - lr: 0.0010\n",
      "Epoch 192/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 35111020.0000 - mae: 1924.2206 - val_loss: 14772802.0000 - val_mae: 2292.7188 - lr: 0.0010\n",
      "Epoch 193/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 34904616.0000 - mae: 1917.2517 - val_loss: 14696123.0000 - val_mae: 2286.0967 - lr: 0.0010\n",
      "Epoch 194/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 34634448.0000 - mae: 1910.5273 - val_loss: 14618558.0000 - val_mae: 2279.3755 - lr: 0.0010\n",
      "Epoch 195/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 34408140.0000 - mae: 1904.2131 - val_loss: 14540793.0000 - val_mae: 2272.6357 - lr: 0.0010\n",
      "Epoch 196/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 34129608.0000 - mae: 1897.2178 - val_loss: 14465324.0000 - val_mae: 2266.4968 - lr: 0.0010\n",
      "Epoch 197/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 33827632.0000 - mae: 1889.4501 - val_loss: 14391487.0000 - val_mae: 2260.5898 - lr: 0.0010\n",
      "Epoch 198/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 33619216.0000 - mae: 1883.1343 - val_loss: 14314884.0000 - val_mae: 2254.3716 - lr: 0.0010\n",
      "Epoch 199/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 33317552.0000 - mae: 1876.9158 - val_loss: 14240446.0000 - val_mae: 2248.2307 - lr: 0.0010\n",
      "Epoch 200/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 33118940.0000 - mae: 1869.7744 - val_loss: 14166644.0000 - val_mae: 2242.3320 - lr: 0.0010\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "2/2 [==============================] - 0s 889us/step\n",
      "  Metric       Train        Test\n",
      "0    MSE 29133984.54 38760396.13\n",
      "1   RMSE     5397.59     6225.78\n",
      "2    MAE     1939.32     2697.93\n",
      "3     R²        0.46        0.54\n",
      "Training time: 3.2962639331817627 seconds\n",
      "Model size: 56 bytes\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# Building neural network models\n",
    "model = Sequential([\n",
    "    Dense(128, input_dim=X_train.shape[1], activation='relu'),  # input layer\n",
    "    Dense(64, activation='relu'),  # hidden layer\n",
    "    Dense(Y_train.shape[1], activation='linear')  # output layer\n",
    "])\n",
    "\n",
    "# Compilation model\n",
    "model.compile(loss='mse', optimizer=Adam(learning_rate=0.001), metrics=['mae'])\n",
    "\n",
    "# Learning rate decay\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=0.00001, verbose=1)\n",
    "\n",
    "# Starting time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model, adjust batch size and number of training rounds\n",
    "history = model.fit(X_train, Y_train, epochs=200, batch_size=20, verbose=1, validation_split=0.2, callbacks=[reduce_lr])\n",
    "\n",
    "# Predictions on the test set using the model\n",
    "Y_pred = model.predict(X_test)\n",
    "\n",
    "# Using the model's predictions on the training set (for calculating training set metrics)\n",
    "Y_train_pred = model.predict(X_train)\n",
    "\n",
    "# Compute various evaluation metrics for training and test sets\n",
    "nn_train_r2 = r2_score(Y_train, Y_train_pred)\n",
    "nn_train_mse = mean_squared_error(Y_train, Y_train_pred)\n",
    "nn_train_rmse = np.sqrt(nn_train_mse)\n",
    "nn_train_mae = mean_absolute_error(Y_train, Y_train_pred)\n",
    "\n",
    "nn_test_r2 = r2_score(Y_test, Y_pred)\n",
    "nn_test_mse = mean_squared_error(Y_test, Y_pred)\n",
    "nn_test_rmse = np.sqrt(nn_test_mse)\n",
    "nn_test_mae = mean_absolute_error(Y_test, Y_pred)\n",
    "\n",
    "# Stopping the timer and calculating the running time\n",
    "nn_end_time = time.time()\n",
    "nn_elapsed_time = nn_end_time - start_time\n",
    "\n",
    "# Creating and displaying indicator tables\n",
    "nn_metrics_df = pd.DataFrame({\n",
    "    'Metric': ['MSE', 'RMSE', 'MAE', 'R²'],\n",
    "    'Train': [nn_train_mse, nn_train_rmse, nn_train_mae, nn_train_r2],\n",
    "    'Test': [nn_test_mse, nn_test_rmse, nn_test_mae, nn_test_r2]\n",
    "})\n",
    "\n",
    "print(nn_metrics_df)\n",
    "\n",
    "# Print out the training time of the model\n",
    "print(\"Training time:\", nn_elapsed_time, \"seconds\")\n",
    "\n",
    "# Calculate and print the memory size occupied by the model\n",
    "print(\"Model size:\", sys.getsizeof(model), \"bytes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Summarize the performance of all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k7/zytrtrv95lv85tshx_l_7cmr0000gn/T/ipykernel_28395/1327469509.py:42: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_df = pd.concat([combined_df, new_row], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MSE Train</th>\n",
       "      <th>MSE Test</th>\n",
       "      <th>R-Squared Train</th>\n",
       "      <th>R-Squared Test</th>\n",
       "      <th>RMSE Train</th>\n",
       "      <th>RMSE Test</th>\n",
       "      <th>MAE Train</th>\n",
       "      <th>MAE Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Model</td>\n",
       "      <td>3799680.28</td>\n",
       "      <td>10279735.70</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.69</td>\n",
       "      <td>1949.28</td>\n",
       "      <td>3206.20</td>\n",
       "      <td>1052.13</td>\n",
       "      <td>1504.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Best Lasso Model</td>\n",
       "      <td>6792162.99</td>\n",
       "      <td>6813601.01</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.77</td>\n",
       "      <td>2606.18</td>\n",
       "      <td>2610.29</td>\n",
       "      <td>1180.93</td>\n",
       "      <td>1282.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Best RF Model</td>\n",
       "      <td>2193164.35</td>\n",
       "      <td>10743368.63</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1480.93</td>\n",
       "      <td>3277.71</td>\n",
       "      <td>536.52</td>\n",
       "      <td>1626.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GB Model</td>\n",
       "      <td>5395.30</td>\n",
       "      <td>25167487.34</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.59</td>\n",
       "      <td>73.45</td>\n",
       "      <td>5016.72</td>\n",
       "      <td>41.73</td>\n",
       "      <td>2194.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Best Neural Network Model</td>\n",
       "      <td>29133984.54</td>\n",
       "      <td>38760396.13</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.54</td>\n",
       "      <td>5397.59</td>\n",
       "      <td>6225.78</td>\n",
       "      <td>1939.32</td>\n",
       "      <td>2697.93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Model   MSE Train    MSE Test  R-Squared Train  \\\n",
       "0               Linear Model  3799680.28 10279735.70             0.85   \n",
       "1           Best Lasso Model  6792162.99  6813601.01             0.78   \n",
       "2              Best RF Model  2193164.35 10743368.63             0.94   \n",
       "3                   GB Model     5395.30 25167487.34             1.00   \n",
       "4  Best Neural Network Model 29133984.54 38760396.13             0.46   \n",
       "\n",
       "   R-Squared Test  RMSE Train  RMSE Test  MAE Train  MAE Test  \n",
       "0            0.69     1949.28    3206.20    1052.13   1504.04  \n",
       "1            0.77     2606.18    2610.29    1180.93   1282.75  \n",
       "2            0.77     1480.93    3277.71     536.52   1626.70  \n",
       "3            0.59       73.45    5016.72      41.73   2194.90  \n",
       "4            0.54     5397.59    6225.78    1939.32   2697.93  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create an empty DataFrame\n",
    "columns = ['Model', 'MSE Train', 'MSE Test', 'R-Squared Train', 'R-Squared Test', \n",
    "           'RMSE Train', 'RMSE Test', 'MAE Train', 'MAE Test']\n",
    "combined_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Populating the DataFrame\n",
    "models = [\n",
    "    ('Linear Model', linear_metrics_df),\n",
    "    ('Best Lasso Model', lasso_metrics_df),\n",
    "    ('Best RF Model', rf_metrics_df),\n",
    "    ('GB Model', gbm_metrics_df),\n",
    "    ('Best Neural Network Model', nn_metrics_df)\n",
    "]\n",
    "\n",
    "for model_name, metrics_df in models:\n",
    "    # Extract the required metrics values from metrics_df\n",
    "    mse_train = metrics_df.loc[metrics_df['Metric'] == 'MSE', 'Train'].values[0]\n",
    "    mse_test = metrics_df.loc[metrics_df['Metric'] == 'MSE', 'Test'].values[0]\n",
    "    r2_train = metrics_df.loc[metrics_df['Metric'] == 'R²', 'Train'].values[0]\n",
    "    r2_test = metrics_df.loc[metrics_df['Metric'] == 'R²', 'Test'].values[0]\n",
    "    rmse_train = metrics_df.loc[metrics_df['Metric'] == 'RMSE', 'Train'].values[0]\n",
    "    rmse_test = metrics_df.loc[metrics_df['Metric'] == 'RMSE', 'Test'].values[0]\n",
    "    mae_train = metrics_df.loc[metrics_df['Metric'] == 'MAE', 'Train'].values[0]\n",
    "    mae_test = metrics_df.loc[metrics_df['Metric'] == 'MAE', 'Test'].values[0]\n",
    "\n",
    "    # Create a new row DataFrame\n",
    "    new_row = pd.DataFrame({\n",
    "        'Model': [model_name],\n",
    "        'MSE Train': [mse_train],\n",
    "        'MSE Test': [mse_test],\n",
    "        'R-Squared Train': [r2_train],\n",
    "        'R-Squared Test': [r2_test],\n",
    "        'RMSE Train': [rmse_train],\n",
    "        'RMSE Test': [rmse_test],\n",
    "        'MAE Train': [mae_train],\n",
    "        'MAE Test': [mae_test]\n",
    "    })\n",
    "\n",
    "    # Use pd.concat to add lines\n",
    "    combined_df = pd.concat([combined_df, new_row], ignore_index=True)\n",
    "\n",
    "# Output the consolidated DataFrame\n",
    "combined_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all models take less time to run and occupy similar amount of memory. So we only rank the performance of each model based on the performance summary and data provided.\n",
    "\n",
    "1. **Best Lasso Model**: Demonstrates the best stability and generalization ability overall, with close performance on the training and test sets, shows strong prediction ability on unknown data, and is suitable for applications that require highly reliable predictions.\n",
    "\n",
    "2. **Best RF Model**: despite the excellent performance on the training set, the performance drop on the test set indicates an overfitting problem.\n",
    "\n",
    "3. **Linear Model**: Despite a drop in performance on the test set, it still demonstrates moderate fitting ability. This model is suitable for quick preliminary analysis, but may require more sophisticated modeling when dealing with more complex data relationships.\n",
    "\n",
    "4. **GB Model**: The almost perfect performance on the training set contrasts with the sharp performance drop on the test set, suggesting that the model creates a strong overfitting problem.\n",
    "\n",
    "5. **Best Neural Network Model**: a relatively poor performer among all models, with poor performance on both the training and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Conclusion\n",
    "\n",
    "This week I have summarized the ranking of all predictive models by compiling their performance in terms of accuracy metrics, runtime and memory usage in the training and test sets. Based on the final results, we can see that the lasso model was the best performer in the combined considerations, while the neural network model was the worst performer. Perhaps the Lasso model will be more practically relevant in the topic of studying the economic indicators of each region to predict the topic strategy of phone scams."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
